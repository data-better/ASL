{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "11ê°•-date_formatter_att_mxnet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/data-better/ASL/blob/master/11%EA%B0%95_date_formatter_att_mxnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJDZ233qMVGt",
        "outputId": "8b75ce52-67eb-4836-aa96-e292bca489e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Sat_Aug_25_21:08:01_CDT_2018\n",
            "Cuda compilation tools, release 10.0, V10.0.130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BSFGW_oLKjT"
      },
      "source": [
        "!pip install mxnet-cu100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMhgQdjjLUeO"
      },
      "source": [
        "!pip install gluon"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyDY2ugXKfMS"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import mxnet as mx\n",
        "from mxnet import nd, autograd, gluon\n",
        "from mxnet.gluon import nn, rnn\n",
        "from mxnet.ndarray.linalg import gemm2\n",
        "import datetime\n",
        "from datetime import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1ZjVdCoKfMW",
        "outputId": "6941ace2-6354-4c5a-f10a-2536fb0fc565",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def padding(chars, maxlen):\n",
        "    if len(chars) < maxlen:\n",
        "        return chars + ' ' * (maxlen - len(chars))\n",
        "    else:\n",
        "        return chars[:maxlen]\n",
        "\n",
        "def gen_date():\n",
        "    rnd = int(np.random.uniform(low = 1000000000, high = 1350000000))\n",
        "    timestamp = datetime.fromtimestamp(rnd)\n",
        "    return str(timestamp.strftime('%Y-%B-%d %a')) # '%Y-%B-%d %H:%M:%S'\n",
        "\n",
        "def format_date(x):\n",
        "    return str(datetime.strptime(x, '%Y-%B-%d %a').strftime('%m/%d/%Y, %A')).lower() #'%H%M%S:%Y%m%d'\n",
        "\n",
        "N = 1000\n",
        "N_train = int(N * .9)\n",
        "N_validation = N - N_train\n",
        "\n",
        "in_seq_len = 32\n",
        "out_seq_len = 32\n",
        "\n",
        "added = set()\n",
        "questions = []\n",
        "answers = []\n",
        "answers_y = []\n",
        "\n",
        "while len(questions) < N:\n",
        "    if len(questions) % 1000 == 0:\n",
        "        print('i = {}'.format(len(questions)))\n",
        "    a = gen_date()\n",
        "    if a in added:\n",
        "        continue\n",
        "    question = '[{}]'.format(a)\n",
        "    answer = '[' + str(format_date(a)) + ']'\n",
        "    answer = padding(answer, out_seq_len)\n",
        "    answer_y = str(format_date(a)) + ']'\n",
        "    answer_y = padding(answer_y, out_seq_len)\n",
        "    \n",
        "    added.add(a)\n",
        "    questions.append(question)\n",
        "    answers.append(answer)\n",
        "    answers_y.append(answer_y)\n",
        "\n",
        "chars = list(set(''.join(questions[:20000])))\n",
        "chars.extend(['[', ']']) # Start and End of Expression\n",
        "chars.extend(list(set(''.join(answers[:20000]))))\n",
        "chars = np.sort(list(set(chars)))\n",
        "\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i = 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBubAXmUKfMZ",
        "outputId": "4a589395-6d81-4e41-b516-ea6f10932e67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(questions[:10])\n",
        "print(answers[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[2009-March-06 Fri]', '[2004-January-17 Sat]', '[2006-February-10 Fri]', '[2003-January-04 Sat]', '[2011-February-14 Mon]', '[2002-March-08 Fri]', '[2010-July-31 Sat]', '[2004-January-31 Sat]', '[2008-June-08 Sun]', '[2002-July-02 Tue]']\n",
            "['[03/06/2009, friday]            ', '[01/17/2004, saturday]          ', '[02/10/2006, friday]            ', '[01/04/2003, saturday]          ', '[02/14/2011, monday]            ', '[03/08/2002, friday]            ', '[07/31/2010, saturday]          ', '[01/31/2004, saturday]          ', '[06/08/2008, sunday]            ', '[07/02/2002, tuesday]           ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdZrhEr4KfMb"
      },
      "source": [
        "X = np.zeros((len(questions), in_seq_len, len(chars)), dtype=np.integer)\n",
        "Y = np.zeros((len(questions), out_seq_len, len(chars)), dtype=np.integer)\n",
        "Z = np.zeros((len(questions), out_seq_len, len(chars)), dtype=np.integer)\n",
        "\n",
        "for i in range(N):\n",
        "    for t, char in enumerate(questions[i]):\n",
        "        X[i, t, char_indices[char]] = 1\n",
        "    for t, char in enumerate(answers[i]):\n",
        "        Y[i, t, char_indices[char]] = 1\n",
        "    for t, char in enumerate(answers_y[i]):\n",
        "        Z[i, t, char_indices[char]] = 1\n",
        "    \n",
        "X_train, X_validation, Y_train, Y_validation, Z_train, Z_validation = \\\n",
        "    train_test_split(X, Y, Z, train_size=N_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3dEjqGcKfMd"
      },
      "source": [
        "### Testset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBtSCGHJKfMe"
      },
      "source": [
        "def gen_test(N):\n",
        "    q = []\n",
        "    y = []\n",
        "    \n",
        "    for i in range(N):\n",
        "        question = gen_date()\n",
        "        answer_y = format_date(question)\n",
        "        q.append(question)\n",
        "        y.append(answer_y)\n",
        "    return(q,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5-EdTHkKfMg",
        "outputId": "a017c022-b212-4aae-97ba-a57767d8257a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "gen_test(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['2004-September-30 Thu', '2010-August-27 Fri'],\n",
              " ['09/30/2004, thursday', '08/27/2010, friday'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1lIftYhKfMi"
      },
      "source": [
        "class colors:\n",
        "    ok = '\\033[92m'\n",
        "    fail = '\\033[91m'\n",
        "    close = '\\033[0m'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIgV_eCXKfMk"
      },
      "source": [
        "class pattern_matcher(gluon.Block):\n",
        "    def __init__(self, n_hidden, in_seq_len, out_seq_len, vocab_size, ctx, **kwargs):\n",
        "        super(pattern_matcher, self).__init__(**kwargs)\n",
        "        self.in_seq_len = in_seq_len\n",
        "        self.out_seq_len = out_seq_len\n",
        "        self.n_hidden = n_hidden\n",
        "        self.vocab_size = vocab_size\n",
        "        \n",
        "        with self.name_scope():\n",
        "            self.encoder = rnn.LSTMCell(hidden_size = n_hidden)\n",
        "            self.decoder = rnn.LSTMCell(hidden_size = n_hidden)\n",
        "            self.attn_weight = nn.Dense(self.in_seq_len, in_units = self.in_seq_len)            \n",
        "            self.batchnorm = nn.BatchNorm(axis = 2)\n",
        "            self.dense = nn.Dense(self.vocab_size, flatten = False)\n",
        "            \n",
        "    def forward(self, inputs, outputs):\n",
        "        self.batch_size = inputs.shape[0]\n",
        "        enout, (next_h, next_c) = self.encoder.unroll(inputs = inputs \\\n",
        "                                                    , length = self.in_seq_len \\\n",
        "                                                    , merge_outputs = True)\n",
        "        # enout: (n_batch * time_step * n_hidden), next_h, next_c: (n_batch * n_hidden)\n",
        "        for i in range(self.out_seq_len):\n",
        "            # For each time step, caclculate context for attention\n",
        "            _n_h = next_h.expand_dims(axis = 2)\n",
        "            ####### Attention part: To get context vector at jth point of output sequence\n",
        "            score_i = gemm2(enout, next_h.expand_dims(axis = 2))  # n_batch * time_step * 1\n",
        "            alpha_i = nd.softmax(self.attn_weight(score_i)) # n_batch * time_step\n",
        "            alpha_expand = alpha_i.expand_dims(2) # (n_batch * 1 * time_step)\n",
        "            alpha_expand = nd.repeat(alpha_expand,repeats= self.n_hidden, axis=2) # n_batch * time_step * n_hidden\n",
        "            context = nd.multiply(alpha_expand, enout) # n_batch * time_step * n_hidden\n",
        "            context = nd.sum(context, axis = 1) # n_batch * n_hidden\n",
        "            _in = nd.concat(outputs[:, i, :], context)\n",
        "            deout, (next_h, next_c) = self.decoder(_in, [next_h, next_c],)\n",
        "            if i == 0:\n",
        "                deouts = deout\n",
        "            else:\n",
        "                deouts = nd.concat(deouts, deout, dim = 1)   \n",
        "        deouts = nd.reshape(deouts, (-1, self.out_seq_len, self.n_hidden))\n",
        "        deouts = self.batchnorm(deouts)\n",
        "        deouts_fc = self.dense(deouts)\n",
        "        return deouts_fc\n",
        "    \n",
        "    def predict(self, input_str, char_indices, indices_char, input_digits = 9, lchars = 14, ctx = mx.gpu()):\n",
        "        # No label when evaluating new example. So try to put the result of the previous time step\n",
        "        alpha = []\n",
        "        input_str = '[' + input_str + ']'\n",
        "        X = nd.zeros((1, input_digits, lchars), ctx = ctx)\n",
        "        for t, char in enumerate(input_str):\n",
        "            X[0, t, char_indices[char]] = 1\n",
        "        Y_init = nd.zeros((1, lchars), ctx = ctx)\n",
        "        Y_init[0, char_indices['[']] = 1\n",
        "        enout, (next_h, next_c) = self.encoder.unroll(inputs = X, length = self.in_seq_len, merge_outputs = True)\n",
        "        deout = Y_init\n",
        "        \n",
        "        for i in range(self.out_seq_len):\n",
        "            ####### Attention part: To get context vector at jth point of output sequence\n",
        "            score_i = gemm2(enout, next_h.expand_dims(axis = 2))\n",
        "            alpha_i = nd.softmax(self.attn_weight(score_i))\n",
        "            alpha_expand = alpha_i.expand_dims(2)\n",
        "            alpha_expand = nd.repeat(alpha_expand,repeats= self.n_hidden, axis=2) # n_batch * time step * n_hidden\n",
        "            context = nd.multiply(alpha_expand, enout)\n",
        "            context = nd.sum(context, axis = 1) # n_batch * n_hidden\n",
        "            _in = nd.concat(deout, context)\n",
        "            deout, (next_h, next_c) = self.decoder(_in, [next_h, next_c],)\n",
        "            deout = nd.expand_dims(deout, axis = 1)\n",
        "            deout = self.batchnorm(deout)\n",
        "            deout = deout[:, 0, :]\n",
        "            deout_sm = self.dense(deout)\n",
        "            deout = nd.one_hot(nd.argmax(nd.softmax(deout_sm, axis = 1), axis = 1), depth = self.vocab_size)\n",
        "            if i == 0:\n",
        "                ret_seq = indices_char[nd.argmax(deout_sm, axis = 1).asnumpy()[0].astype('int')]\n",
        "            else:\n",
        "                ret_seq += indices_char[nd.argmax(deout_sm, axis = 1).asnumpy()[0].astype('int')]\n",
        "            if ret_seq[-1] == ']':\n",
        "                break\n",
        "            alpha.append(alpha_i.asnumpy())\n",
        "        return ret_seq.strip(']').strip(), np.squeeze(np.array(alpha), axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKd5dOLYKfMm"
      },
      "source": [
        "tr_set = gluon.data.ArrayDataset(X_train, Y_train, Z_train)\n",
        "tr_data_iterator = gluon.data.DataLoader(tr_set, batch_size=256, shuffle=True)\n",
        "\n",
        "te_set =gluon.data.ArrayDataset(X_validation, Y_validation, Z_validation)\n",
        "te_data_iterator = gluon.data.DataLoader(te_set, batch_size=256, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "097DKuKyKfMo"
      },
      "source": [
        "ctx = mx.gpu()\n",
        "model = pattern_matcher(300, in_seq_len, out_seq_len, len(chars), ctx)\n",
        "model.collect_params().initialize(mx.init.Xavier(), ctx = ctx)\n",
        "\n",
        "trainer = gluon.Trainer(model.collect_params(), 'rmsprop')\n",
        "loss = gluon.loss.SoftmaxCrossEntropyLoss(axis = 2, sparse_label = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaNOSJYNKfMq",
        "outputId": "468ef83e-e951-46d3-fc3c-89b8055d0f34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pattern_matcher(\n",
            "  (encoder): LSTMCell(None -> 1200)\n",
            "  (decoder): LSTMCell(None -> 1200)\n",
            "  (attn_weight): Dense(32 -> 32, linear)\n",
            "  (batchnorm): BatchNorm(axis=2, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
            "  (dense): Dense(None -> 47, linear)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0g2roTqKfMs"
      },
      "source": [
        "def calculate_loss(model, data_iter, loss_obj, ctx = ctx):\n",
        "    test_loss = []\n",
        "    for i, (x_data, y_data, z_data) in enumerate(data_iter):\n",
        "        x_data = x_data.as_in_context(ctx).astype('float32')\n",
        "        y_data = y_data.as_in_context(ctx).astype('float32')\n",
        "        z_data = z_data.as_in_context(ctx).astype('float32')\n",
        "        with autograd.predict_mode():\n",
        "            z_output = model(x_data, y_data)\n",
        "            loss_te = loss_obj(z_output, z_data)\n",
        "        curr_loss = nd.mean(loss_te).asscalar()\n",
        "        test_loss.append(curr_loss)\n",
        "    return np.mean(test_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIvcnGjVKfMv",
        "outputId": "2d0a4cba-5f6b-4ec7-d6eb-e05f88147dac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epochs = 200\n",
        "\n",
        "tot_test_loss = []\n",
        "tot_train_loss = []\n",
        "for e in range(epochs):\n",
        "    train_loss = []\n",
        "    for i, (x_data, y_data, z_data) in enumerate(tr_data_iterator):\n",
        "        x_data = x_data.as_in_context(ctx).astype('float32')\n",
        "        y_data = y_data.as_in_context(ctx).astype('float32')\n",
        "        z_data = z_data.as_in_context(ctx).astype('float32')\n",
        "        \n",
        "        with autograd.record():\n",
        "            z_output = model(x_data, y_data)\n",
        "            loss_ = loss(z_output, z_data)\n",
        "        loss_.backward()\n",
        "        trainer.step(x_data.shape[0])\n",
        "        curr_loss = nd.mean(loss_).asscalar()\n",
        "        train_loss.append(curr_loss)\n",
        "        \n",
        "    if e % 10 == 0:\n",
        "        q, y = gen_test(10)\n",
        "        for i in range(10):\n",
        "            with autograd.predict_mode():\n",
        "                p, attn = model.predict(q[i], char_indices, indices_char, input_digits = in_seq_len, lchars = len(chars))\n",
        "                p = p.strip()\n",
        "                iscorr = 1 if p == y[i] else 0\n",
        "                if iscorr == 1:\n",
        "                    print(colors.ok + 'â˜‘' + colors.close, end=' ')\n",
        "                else:\n",
        "                    print(colors.fail + 'â˜’' + colors.close, end=' ')\n",
        "                print(\"{} = {}({}) {}, attention {}\".format(q[i], p, y[i], str(iscorr), attn.shape))\n",
        "    #caculate test loss\n",
        "    test_loss = calculate_loss(model, te_data_iterator, loss_obj = loss, ctx=ctx) \n",
        "\n",
        "    print(\"Epoch %s. Train Loss: %s, Test Loss : %s\" % (e, np.mean(train_loss), test_loss))    \n",
        "    tot_test_loss.append(test_loss)\n",
        "    tot_train_loss.append(np.mean(train_loss))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[92mâ˜‘\u001b[0m 2006-June-24 Sat = 06/24/2006, saturday(06/24/2006, saturday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2006-July-26 Wed = 07/26/2006, wednesday(07/26/2006, wednesday) 1, attention (21, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2001-September-09 Sun = 09/09/2001, sunday(09/09/2001, sunday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2006-August-08 Tue = 08/08/2006, tuesday(08/08/2006, tuesday) 1, attention (19, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2011-April-25 Mon = 04/25/2011, monday(04/25/2011, monday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2008-January-27 Sun = 01/27/2008, sunday(01/27/2008, sunday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2004-October-13 Wed = 10/13/2004, wednesday(10/13/2004, wednesday) 1, attention (21, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2008-March-19 Wed = 03/19/2008, wednesday(03/19/2008, wednesday) 1, attention (21, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2007-May-19 Sat = 05/19/2007, saturday(05/19/2007, saturday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2010-January-21 Thu = 01/21/2010, thursday(01/21/2010, thursday) 1, attention (20, 32)\n",
            "Epoch 0. Train Loss: 2.2899058e-05, Test Loss : 0.00014510119\n",
            "Epoch 1. Train Loss: 2.2445656e-05, Test Loss : 0.00013680905\n",
            "Epoch 2. Train Loss: 2.3017323e-05, Test Loss : 0.0001271791\n",
            "Epoch 3. Train Loss: 2.2289272e-05, Test Loss : 0.00013999915\n",
            "Epoch 4. Train Loss: 2.3053552e-05, Test Loss : 0.00014155892\n",
            "Epoch 5. Train Loss: 2.2006787e-05, Test Loss : 0.0001327304\n",
            "Epoch 6. Train Loss: 2.3066004e-05, Test Loss : 0.00012820674\n",
            "Epoch 7. Train Loss: 2.2769584e-05, Test Loss : 0.00014514479\n",
            "Epoch 8. Train Loss: 2.381579e-05, Test Loss : 0.00014201697\n",
            "Epoch 9. Train Loss: 2.2448206e-05, Test Loss : 0.0001353151\n",
            "\u001b[92mâ˜‘\u001b[0m 2008-September-22 Mon = 09/22/2008, monday(09/22/2008, monday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2002-July-03 Wed = 07/03/2002, wednesday(07/03/2002, wednesday) 1, attention (21, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2012-April-07 Sat = 04/07/2012, saturday(04/07/2012, saturday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2011-February-27 Sun = 02/27/2011, sunday(02/27/2011, sunday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2012-March-05 Mon = 03/05/2012, monday(03/05/2012, monday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2005-September-11 Sun = 09/11/2005, sunday(09/11/2005, sunday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2008-February-05 Tue = 02/05/2008, tuesday(02/05/2008, tuesday) 1, attention (19, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2008-November-17 Mon = 11/17/2008, monday(11/17/2008, monday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2003-July-26 Sat = 07/26/2003, saturday(07/26/2003, saturday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2010-May-16 Sun = 05/16/2010, sunday(05/16/2010, sunday) 1, attention (18, 32)\n",
            "Epoch 10. Train Loss: 2.2133943e-05, Test Loss : 0.00012360788\n",
            "Epoch 11. Train Loss: 2.2256438e-05, Test Loss : 0.00012664679\n",
            "Epoch 12. Train Loss: 2.1636624e-05, Test Loss : 0.00013985485\n",
            "Epoch 13. Train Loss: 2.1870286e-05, Test Loss : 0.00013750573\n",
            "Epoch 14. Train Loss: 2.2994102e-05, Test Loss : 0.00013612755\n",
            "Epoch 15. Train Loss: 2.1016054e-05, Test Loss : 0.00013573197\n",
            "Epoch 16. Train Loss: 2.1732987e-05, Test Loss : 0.000145325\n",
            "Epoch 17. Train Loss: 2.242482e-05, Test Loss : 0.00014991657\n",
            "Epoch 18. Train Loss: 2.331436e-05, Test Loss : 0.00018343264\n",
            "Epoch 19. Train Loss: 2.0736381e-05, Test Loss : 0.0001592325\n",
            "\u001b[92mâ˜‘\u001b[0m 2005-July-21 Thu = 07/21/2005, thursday(07/21/2005, thursday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2010-February-03 Wed = 02/03/2010, wednesday(02/03/2010, wednesday) 1, attention (21, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2002-November-10 Sun = 11/10/2002, sunday(11/10/2002, sunday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2004-January-06 Tue = 01/06/2004, tuesday(01/06/2004, tuesday) 1, attention (19, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2006-September-21 Thu = 09/21/2006, thursday(09/21/2006, thursday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2008-November-02 Sun = 11/02/2008, sunday(11/02/2008, sunday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2007-December-23 Sun = 12/23/2007, sunday(12/23/2007, sunday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2006-March-12 Sun = 03/12/2006, sunday(03/12/2006, sunday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2011-November-18 Fri = 11/18/2011, friday(11/18/2011, friday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2004-April-09 Fri = 04/09/2004, friday(04/09/2004, friday) 1, attention (18, 32)\n",
            "Epoch 20. Train Loss: 2.1439777e-05, Test Loss : 0.0001346214\n",
            "Epoch 21. Train Loss: 2.1350465e-05, Test Loss : 0.00012568627\n",
            "Epoch 22. Train Loss: 2.0290503e-05, Test Loss : 0.00012549816\n",
            "Epoch 23. Train Loss: 2.189679e-05, Test Loss : 0.00012539799\n",
            "Epoch 24. Train Loss: 2.1531669e-05, Test Loss : 0.00012532533\n",
            "Epoch 25. Train Loss: 2.1707612e-05, Test Loss : 0.00013299432\n",
            "Epoch 26. Train Loss: 2.4071951e-05, Test Loss : 0.0003685042\n",
            "Epoch 27. Train Loss: 2.1703907e-05, Test Loss : 0.00014172698\n",
            "Epoch 28. Train Loss: 2.0725613e-05, Test Loss : 0.00012896444\n",
            "Epoch 29. Train Loss: 2.029443e-05, Test Loss : 0.00014799289\n",
            "\u001b[92mâ˜‘\u001b[0m 2008-June-14 Sat = 06/14/2008, saturday(06/14/2008, saturday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2007-September-21 Fri = 09/21/2007, friday(09/21/2007, friday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2007-February-19 Mon = 02/19/2007, monday(02/19/2007, monday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2012-May-26 Sat = 05/26/2012, saturday(05/26/2012, saturday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2004-May-04 Tue = 05/04/2004, tuesday(05/04/2004, tuesday) 1, attention (19, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2011-January-16 Sun = 01/16/2011, sunday(01/16/2011, sunday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2003-September-25 Thu = 09/25/2003, thursday(09/25/2003, thursday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2012-June-03 Sun = 06/03/2012, sunday(06/03/2012, sunday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2002-June-20 Thu = 06/20/2002, thursday(06/20/2002, thursday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2006-January-19 Thu = 01/19/2006, thursday(01/19/2006, thursday) 1, attention (20, 32)\n",
            "Epoch 30. Train Loss: 2.0175932e-05, Test Loss : 0.00012898965\n",
            "Epoch 31. Train Loss: 2.095519e-05, Test Loss : 0.00013269656\n",
            "Epoch 32. Train Loss: 2.1368676e-05, Test Loss : 0.00012713086\n",
            "Epoch 33. Train Loss: 2.1850425e-05, Test Loss : 0.0001533511\n",
            "Epoch 34. Train Loss: 2.104374e-05, Test Loss : 0.00015756619\n",
            "Epoch 35. Train Loss: 2.010298e-05, Test Loss : 0.00012331507\n",
            "Epoch 36. Train Loss: 1.9444118e-05, Test Loss : 0.00012976781\n",
            "Epoch 37. Train Loss: 2.0219944e-05, Test Loss : 0.00013857438\n",
            "Epoch 38. Train Loss: 2.3386921e-05, Test Loss : 0.0002928505\n",
            "Epoch 39. Train Loss: 2.0769294e-05, Test Loss : 0.0001253697\n",
            "\u001b[92mâ˜‘\u001b[0m 2005-August-30 Tue = 08/30/2005, tuesday(08/30/2005, tuesday) 1, attention (19, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2012-September-22 Sat = 09/22/2012, saturday(09/22/2012, saturday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2010-January-22 Fri = 01/22/2010, friday(01/22/2010, friday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2006-January-26 Thu = 01/26/2006, thursday(01/26/2006, thursday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2008-June-08 Sun = 06/08/2008, sunday(06/08/2008, sunday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2003-April-28 Mon = 04/28/2003, monday(04/28/2003, monday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2010-July-25 Sun = 07/25/2010, sunday(07/25/2010, sunday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2002-February-27 Wed = 02/27/2002, wednesday(02/27/2002, wednesday) 1, attention (21, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2011-July-10 Sun = 07/10/2011, sunday(07/10/2011, sunday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2003-June-05 Thu = 06/05/2003, thursday(06/05/2003, thursday) 1, attention (20, 32)\n",
            "Epoch 40. Train Loss: 2.080786e-05, Test Loss : 0.00015649851\n",
            "Epoch 41. Train Loss: 1.9356e-05, Test Loss : 0.00012226781\n",
            "Epoch 42. Train Loss: 2.2781029e-05, Test Loss : 0.0004807459\n",
            "Epoch 43. Train Loss: 2.023124e-05, Test Loss : 0.00014053416\n",
            "Epoch 44. Train Loss: 1.9428742e-05, Test Loss : 0.00012970447\n",
            "Epoch 45. Train Loss: 1.9229528e-05, Test Loss : 0.00012072323\n",
            "Epoch 46. Train Loss: 2.0143618e-05, Test Loss : 0.00011510109\n",
            "Epoch 47. Train Loss: 1.9934605e-05, Test Loss : 0.00012741867\n",
            "Epoch 48. Train Loss: 1.9222287e-05, Test Loss : 0.00011524658\n",
            "Epoch 49. Train Loss: 1.8945044e-05, Test Loss : 0.00011757918\n",
            "\u001b[92mâ˜‘\u001b[0m 2006-February-28 Tue = 02/28/2006, tuesday(02/28/2006, tuesday) 1, attention (19, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2001-September-14 Fri = 09/14/2001, friday(09/14/2001, friday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2006-October-02 Mon = 10/02/2006, monday(10/02/2006, monday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2010-April-06 Tue = 04/06/2010, tuesday(04/06/2010, tuesday) 1, attention (19, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2009-January-22 Thu = 01/22/2009, thursday(01/22/2009, thursday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2008-June-29 Sun = 06/29/2008, sunday(06/29/2008, sunday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2009-April-26 Sun = 04/26/2009, sunday(04/26/2009, sunday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2009-March-05 Thu = 03/05/2009, thursday(03/05/2009, thursday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2004-January-05 Mon = 01/05/2004, monday(01/05/2004, monday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2001-October-18 Thu = 10/18/2001, thursday(10/18/2001, thursday) 1, attention (20, 32)\n",
            "Epoch 50. Train Loss: 1.9520336e-05, Test Loss : 0.00012863794\n",
            "Epoch 51. Train Loss: 1.967358e-05, Test Loss : 0.00014116208\n",
            "Epoch 52. Train Loss: 1.8606923e-05, Test Loss : 0.00011977201\n",
            "Epoch 53. Train Loss: 2.157804e-05, Test Loss : 0.00012217788\n",
            "Epoch 54. Train Loss: 1.8966084e-05, Test Loss : 0.00013302322\n",
            "Epoch 55. Train Loss: 1.8635928e-05, Test Loss : 0.00011838545\n",
            "Epoch 56. Train Loss: 1.989624e-05, Test Loss : 0.0001222896\n",
            "Epoch 57. Train Loss: 1.9643656e-05, Test Loss : 0.0001279537\n",
            "Epoch 58. Train Loss: 1.9107725e-05, Test Loss : 0.00013117176\n",
            "Epoch 59. Train Loss: 1.8224782e-05, Test Loss : 0.00011827816\n",
            "\u001b[92mâ˜‘\u001b[0m 2002-December-13 Fri = 12/13/2002, friday(12/13/2002, friday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2006-February-23 Thu = 02/23/2006, thursday(02/23/2006, thursday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2003-April-24 Thu = 04/24/2003, thursday(04/24/2003, thursday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2008-May-13 Tue = 05/13/2008, tuesday(05/13/2008, tuesday) 1, attention (19, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2004-August-13 Fri = 08/13/2004, friday(08/13/2004, friday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2008-March-02 Sun = 03/02/2008, sunday(03/02/2008, sunday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2011-July-27 Wed = 07/27/2011, wednesday(07/27/2011, wednesday) 1, attention (21, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2006-December-07 Thu = 12/07/2006, thursday(12/07/2006, thursday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2004-May-27 Thu = 05/27/2004, thursday(05/27/2004, thursday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2006-January-14 Sat = 01/14/2006, saturday(01/14/2006, saturday) 1, attention (20, 32)\n",
            "Epoch 60. Train Loss: 1.8294419e-05, Test Loss : 0.00011369596\n",
            "Epoch 61. Train Loss: 1.7958611e-05, Test Loss : 0.000115214156\n",
            "Epoch 62. Train Loss: 1.7811157e-05, Test Loss : 0.00010715624\n",
            "Epoch 63. Train Loss: 1.7590486e-05, Test Loss : 0.00011519402\n",
            "Epoch 64. Train Loss: 1.8238263e-05, Test Loss : 0.000108735396\n",
            "Epoch 65. Train Loss: 1.8373994e-05, Test Loss : 0.000111045374\n",
            "Epoch 66. Train Loss: 1.7762808e-05, Test Loss : 0.00011593919\n",
            "Epoch 67. Train Loss: 1.8343819e-05, Test Loss : 0.00011854716\n",
            "Epoch 68. Train Loss: 2.162428e-05, Test Loss : 0.0001712149\n",
            "Epoch 69. Train Loss: 2.0475827e-05, Test Loss : 0.00010846232\n",
            "\u001b[92mâ˜‘\u001b[0m 2003-July-12 Sat = 07/12/2003, saturday(07/12/2003, saturday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2002-February-13 Wed = 02/13/2002, wednesday(02/13/2002, wednesday) 1, attention (21, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2009-February-07 Sat = 02/07/2009, saturday(02/07/2009, saturday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2006-March-22 Wed = 03/22/2006, wednesday(03/22/2006, wednesday) 1, attention (21, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2001-November-11 Sun = 11/11/2001, sunday(11/11/2001, sunday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2008-May-09 Fri = 05/09/2008, friday(05/09/2008, friday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2002-March-16 Sat = 03/16/2002, saturday(03/16/2002, saturday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2006-May-04 Thu = 05/04/2006, thursday(05/04/2006, thursday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2003-September-28 Sun = 09/28/2003, sunday(09/28/2003, sunday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2007-January-29 Mon = 01/29/2007, monday(01/29/2007, monday) 1, attention (18, 32)\n",
            "Epoch 70. Train Loss: 1.8434359e-05, Test Loss : 0.0001259089\n",
            "Epoch 71. Train Loss: 2.1148535e-05, Test Loss : 0.00012436177\n",
            "Epoch 72. Train Loss: 2.1248097e-05, Test Loss : 0.00013306826\n",
            "Epoch 73. Train Loss: 1.7666653e-05, Test Loss : 0.00010990654\n",
            "Epoch 74. Train Loss: 1.950614e-05, Test Loss : 0.00013206695\n",
            "Epoch 75. Train Loss: 1.8715018e-05, Test Loss : 0.00010946899\n",
            "Epoch 76. Train Loss: 1.8676554e-05, Test Loss : 0.00014245593\n",
            "Epoch 77. Train Loss: 1.7344471e-05, Test Loss : 0.000105520354\n",
            "Epoch 78. Train Loss: 1.681442e-05, Test Loss : 0.00011252236\n",
            "Epoch 79. Train Loss: 1.6608044e-05, Test Loss : 0.00010820408\n",
            "\u001b[92mâ˜‘\u001b[0m 2002-April-09 Tue = 04/09/2002, tuesday(04/09/2002, tuesday) 1, attention (19, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2002-March-15 Fri = 03/15/2002, friday(03/15/2002, friday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2012-September-03 Mon = 09/03/2012, monday(09/03/2012, monday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2009-March-29 Sun = 03/29/2009, sunday(03/29/2009, sunday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2002-January-25 Fri = 01/25/2002, friday(01/25/2002, friday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2009-November-17 Tue = 11/17/2009, tuesday(11/17/2009, tuesday) 1, attention (19, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2006-December-02 Sat = 12/02/2006, saturday(12/02/2006, saturday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2011-December-02 Fri = 12/02/2011, friday(12/02/2011, friday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2012-June-21 Thu = 06/21/2012, thursday(06/21/2012, thursday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2007-January-22 Mon = 01/22/2007, monday(01/22/2007, monday) 1, attention (18, 32)\n",
            "Epoch 80. Train Loss: 1.7163886e-05, Test Loss : 0.00011501218\n",
            "Epoch 81. Train Loss: 1.7752609e-05, Test Loss : 0.00010482773\n",
            "Epoch 82. Train Loss: 1.8037697e-05, Test Loss : 0.00016847151\n",
            "Epoch 83. Train Loss: 1.8896393e-05, Test Loss : 0.000116467985\n",
            "Epoch 84. Train Loss: 1.8616438e-05, Test Loss : 0.00011360167\n",
            "Epoch 85. Train Loss: 1.8018138e-05, Test Loss : 0.000118619355\n",
            "Epoch 86. Train Loss: 1.643999e-05, Test Loss : 0.0001135895\n",
            "Epoch 87. Train Loss: 1.7055663e-05, Test Loss : 0.00011938614\n",
            "Epoch 88. Train Loss: 1.6998982e-05, Test Loss : 0.00010909397\n",
            "Epoch 89. Train Loss: 1.6901642e-05, Test Loss : 0.00011115817\n",
            "\u001b[92mâ˜‘\u001b[0m 2003-January-20 Mon = 01/20/2003, monday(01/20/2003, monday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2010-September-17 Fri = 09/17/2010, friday(09/17/2010, friday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2004-November-12 Fri = 11/12/2004, friday(11/12/2004, friday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2011-August-28 Sun = 08/28/2011, sunday(08/28/2011, sunday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2008-February-18 Mon = 02/18/2008, monday(02/18/2008, monday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2003-June-04 Wed = 06/04/2003, wednesday(06/04/2003, wednesday) 1, attention (21, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2003-July-09 Wed = 07/09/2003, wednesday(07/09/2003, wednesday) 1, attention (21, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2005-July-26 Tue = 07/26/2005, tuesday(07/26/2005, tuesday) 1, attention (19, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2008-July-31 Thu = 07/31/2008, thursday(07/31/2008, thursday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2005-June-08 Wed = 06/08/2005, wednesday(06/08/2005, wednesday) 1, attention (21, 32)\n",
            "Epoch 90. Train Loss: 1.6520531e-05, Test Loss : 0.00011832486\n",
            "Epoch 91. Train Loss: 1.7466326e-05, Test Loss : 0.0001196487\n",
            "Epoch 92. Train Loss: 1.6530492e-05, Test Loss : 0.000107646476\n",
            "Epoch 93. Train Loss: 1.7258186e-05, Test Loss : 0.00013421547\n",
            "Epoch 94. Train Loss: 1.6215501e-05, Test Loss : 0.00011619123\n",
            "Epoch 95. Train Loss: 1.6791884e-05, Test Loss : 0.000115986404\n",
            "Epoch 96. Train Loss: 1.6317217e-05, Test Loss : 0.000111235575\n",
            "Epoch 97. Train Loss: 1.9897874e-05, Test Loss : 0.0002194305\n",
            "Epoch 98. Train Loss: 1.6084192e-05, Test Loss : 0.000106199535\n",
            "Epoch 99. Train Loss: 1.677453e-05, Test Loss : 9.9316865e-05\n",
            "\u001b[92mâ˜‘\u001b[0m 2011-January-11 Tue = 01/11/2011, tuesday(01/11/2011, tuesday) 1, attention (19, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2008-March-17 Mon = 03/17/2008, monday(03/17/2008, monday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2001-November-22 Thu = 11/22/2001, thursday(11/22/2001, thursday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2005-July-22 Fri = 07/22/2005, friday(07/22/2005, friday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2003-February-22 Sat = 02/22/2003, saturday(02/22/2003, saturday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2012-June-13 Wed = 06/13/2012, wednesday(06/13/2012, wednesday) 1, attention (21, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2002-August-27 Tue = 08/27/2002, tuesday(08/27/2002, tuesday) 1, attention (19, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2007-May-27 Sun = 05/27/2007, sunday(05/27/2007, sunday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2009-January-17 Sat = 01/17/2009, saturday(01/17/2009, saturday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2003-March-06 Thu = 03/06/2003, thursday(03/06/2003, thursday) 1, attention (20, 32)\n",
            "Epoch 100. Train Loss: 1.5449423e-05, Test Loss : 9.9577206e-05\n",
            "Epoch 101. Train Loss: 1.6202128e-05, Test Loss : 0.000108437715\n",
            "Epoch 102. Train Loss: 1.5628832e-05, Test Loss : 0.0001307888\n",
            "Epoch 103. Train Loss: 1.7090255e-05, Test Loss : 0.00010041479\n",
            "Epoch 104. Train Loss: 1.7672668e-05, Test Loss : 0.00010674123\n",
            "Epoch 105. Train Loss: 1.7756014e-05, Test Loss : 0.00011492797\n",
            "Epoch 106. Train Loss: 1.514181e-05, Test Loss : 0.00011590539\n",
            "Epoch 107. Train Loss: 1.8534269e-05, Test Loss : 0.00010122528\n",
            "Epoch 108. Train Loss: 1.6243443e-05, Test Loss : 0.00010920697\n",
            "Epoch 109. Train Loss: 1.6141003e-05, Test Loss : 0.0001017255\n",
            "\u001b[92mâ˜‘\u001b[0m 2010-January-01 Fri = 01/01/2010, friday(01/01/2010, friday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2011-February-05 Sat = 02/05/2011, saturday(02/05/2011, saturday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2004-March-14 Sun = 03/14/2004, sunday(03/14/2004, sunday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2012-May-07 Mon = 05/07/2012, monday(05/07/2012, monday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2007-March-25 Sun = 03/25/2007, sunday(03/25/2007, sunday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2003-December-31 Wed = 12/31/2003, wednesday(12/31/2003, wednesday) 1, attention (21, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2003-July-30 Wed = 07/30/2003, wednesday(07/30/2003, wednesday) 1, attention (21, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2012-February-05 Sun = 02/05/2012, sunday(02/05/2012, sunday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2007-August-26 Sun = 08/26/2007, sunday(08/26/2007, sunday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2005-June-22 Wed = 06/22/2005, wednesday(06/22/2005, wednesday) 1, attention (21, 32)\n",
            "Epoch 110. Train Loss: 1.5399622e-05, Test Loss : 0.000120751094\n",
            "Epoch 111. Train Loss: 1.7668459e-05, Test Loss : 0.00010644571\n",
            "Epoch 112. Train Loss: 1.5424732e-05, Test Loss : 0.00012047303\n",
            "Epoch 113. Train Loss: 1.528486e-05, Test Loss : 9.72701e-05\n",
            "Epoch 114. Train Loss: 1.5240107e-05, Test Loss : 0.00010032229\n",
            "Epoch 115. Train Loss: 1.7216807e-05, Test Loss : 0.00012554735\n",
            "Epoch 116. Train Loss: 1.5263464e-05, Test Loss : 0.00010056784\n",
            "Epoch 117. Train Loss: 1.6176316e-05, Test Loss : 0.000106602674\n",
            "Epoch 118. Train Loss: 1.54021e-05, Test Loss : 0.00012306373\n",
            "Epoch 119. Train Loss: 1.4471194e-05, Test Loss : 0.00011537827\n",
            "\u001b[92mâ˜‘\u001b[0m 2008-May-28 Wed = 05/28/2008, wednesday(05/28/2008, wednesday) 1, attention (21, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2002-October-11 Fri = 10/11/2002, friday(10/11/2002, friday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2008-February-02 Sat = 02/02/2008, saturday(02/02/2008, saturday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2004-January-15 Thu = 01/15/2004, thursday(01/15/2004, thursday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2005-November-13 Sun = 11/13/2005, sunday(11/13/2005, sunday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2003-June-14 Sat = 06/14/2003, saturday(06/14/2003, saturday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2004-July-17 Sat = 07/17/2004, saturday(07/17/2004, saturday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2002-June-29 Sat = 06/29/2002, saturday(06/29/2002, saturday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2012-April-02 Mon = 04/02/2012, monday(04/02/2012, monday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2006-November-23 Thu = 11/23/2006, thursday(11/23/2006, thursday) 1, attention (20, 32)\n",
            "Epoch 120. Train Loss: 1.5932412e-05, Test Loss : 0.00010337703\n",
            "Epoch 121. Train Loss: 1.6096747e-05, Test Loss : 0.0001059533\n",
            "Epoch 122. Train Loss: 1.592726e-05, Test Loss : 9.276155e-05\n",
            "Epoch 123. Train Loss: 1.5060889e-05, Test Loss : 9.210894e-05\n",
            "Epoch 124. Train Loss: 1.5691056e-05, Test Loss : 0.00010452263\n",
            "Epoch 125. Train Loss: 1.4798379e-05, Test Loss : 0.00012338544\n",
            "Epoch 126. Train Loss: 1.4230892e-05, Test Loss : 0.00010817348\n",
            "Epoch 127. Train Loss: 1.4598219e-05, Test Loss : 0.00010177779\n",
            "Epoch 128. Train Loss: 1.4820653e-05, Test Loss : 0.00010071428\n",
            "Epoch 129. Train Loss: 1.5137493e-05, Test Loss : 9.892825e-05\n",
            "\u001b[92mâ˜‘\u001b[0m 2008-January-01 Tue = 01/01/2008, tuesday(01/01/2008, tuesday) 1, attention (19, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2004-March-06 Sat = 03/06/2004, saturday(03/06/2004, saturday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2003-December-08 Mon = 12/08/2003, monday(12/08/2003, monday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2005-May-29 Sun = 05/29/2005, sunday(05/29/2005, sunday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2012-September-05 Wed = 09/05/2012, wednesday(09/05/2012, wednesday) 1, attention (21, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2010-October-23 Sat = 10/23/2010, saturday(10/23/2010, saturday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2010-August-28 Sat = 08/28/2010, saturday(08/28/2010, saturday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2009-June-20 Sat = 06/20/2009, saturday(06/20/2009, saturday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2009-March-01 Sun = 03/01/2009, sunday(03/01/2009, sunday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2008-March-09 Sun = 03/09/2008, sunday(03/09/2008, sunday) 1, attention (18, 32)\n",
            "Epoch 130. Train Loss: 1.4974456e-05, Test Loss : 0.00010626238\n",
            "Epoch 131. Train Loss: 1.42742065e-05, Test Loss : 0.00010090458\n",
            "Epoch 132. Train Loss: 1.4536912e-05, Test Loss : 0.000104364386\n",
            "Epoch 133. Train Loss: 1.4576583e-05, Test Loss : 0.00011275194\n",
            "Epoch 134. Train Loss: 1.5275225e-05, Test Loss : 0.00010434716\n",
            "Epoch 135. Train Loss: 1.6333259e-05, Test Loss : 0.00010107993\n",
            "Epoch 136. Train Loss: 1.4537112e-05, Test Loss : 9.6579206e-05\n",
            "Epoch 137. Train Loss: 1.5046917e-05, Test Loss : 0.000103330174\n",
            "Epoch 138. Train Loss: 1.4186247e-05, Test Loss : 9.763243e-05\n",
            "Epoch 139. Train Loss: 1.4232187e-05, Test Loss : 9.42874e-05\n",
            "\u001b[92mâ˜‘\u001b[0m 2005-September-24 Sat = 09/24/2005, saturday(09/24/2005, saturday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2007-April-08 Sun = 04/08/2007, sunday(04/08/2007, sunday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2009-August-04 Tue = 08/04/2009, tuesday(08/04/2009, tuesday) 1, attention (19, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2008-October-09 Thu = 10/09/2008, thursday(10/09/2008, thursday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2006-February-02 Thu = 02/02/2006, thursday(02/02/2006, thursday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2010-March-16 Tue = 03/16/2010, tuesday(03/16/2010, tuesday) 1, attention (19, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2003-April-04 Fri = 04/04/2003, friday(04/04/2003, friday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2010-January-21 Thu = 01/21/2010, thursday(01/21/2010, thursday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2003-July-31 Thu = 07/31/2003, thursday(07/31/2003, thursday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2011-August-10 Wed = 08/10/2011, wednesday(08/10/2011, wednesday) 1, attention (21, 32)\n",
            "Epoch 140. Train Loss: 1.4475168e-05, Test Loss : 0.00010555142\n",
            "Epoch 141. Train Loss: 1.5113967e-05, Test Loss : 0.0001188798\n",
            "Epoch 142. Train Loss: 1.3833574e-05, Test Loss : 9.9362835e-05\n",
            "Epoch 143. Train Loss: 1.3825666e-05, Test Loss : 9.6520285e-05\n",
            "Epoch 144. Train Loss: 1.5603297e-05, Test Loss : 0.000117431126\n",
            "Epoch 145. Train Loss: 1.6309154e-05, Test Loss : 0.00014543948\n",
            "Epoch 146. Train Loss: 1.5081675e-05, Test Loss : 9.9370765e-05\n",
            "Epoch 147. Train Loss: 1.5965603e-05, Test Loss : 0.00015207437\n",
            "Epoch 148. Train Loss: 1.56874e-05, Test Loss : 0.00013565933\n",
            "Epoch 149. Train Loss: 1.5020167e-05, Test Loss : 0.000115506155\n",
            "\u001b[92mâ˜‘\u001b[0m 2003-January-08 Wed = 01/08/2003, wednesday(01/08/2003, wednesday) 1, attention (21, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2011-September-29 Thu = 09/29/2011, thursday(09/29/2011, thursday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2009-December-02 Wed = 12/02/2009, wednesday(12/02/2009, wednesday) 1, attention (21, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2008-January-06 Sun = 01/06/2008, sunday(01/06/2008, sunday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2010-December-31 Fri = 12/31/2010, friday(12/31/2010, friday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2002-February-25 Mon = 02/25/2002, monday(02/25/2002, monday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2007-July-29 Sun = 07/29/2007, sunday(07/29/2007, sunday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2009-January-31 Sat = 01/31/2009, saturday(01/31/2009, saturday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2012-July-13 Fri = 07/13/2012, friday(07/13/2012, friday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2004-April-22 Thu = 04/22/2004, thursday(04/22/2004, thursday) 1, attention (20, 32)\n",
            "Epoch 150. Train Loss: 1.4214493e-05, Test Loss : 9.701557e-05\n",
            "Epoch 151. Train Loss: 1.4784313e-05, Test Loss : 0.000105656545\n",
            "Epoch 152. Train Loss: 1.4025494e-05, Test Loss : 9.972097e-05\n",
            "Epoch 153. Train Loss: 2.0613987e-05, Test Loss : 0.0007880916\n",
            "Epoch 154. Train Loss: 1.6384054e-05, Test Loss : 8.859299e-05\n",
            "Epoch 155. Train Loss: 1.5656518e-05, Test Loss : 8.665012e-05\n",
            "Epoch 156. Train Loss: 1.3208706e-05, Test Loss : 8.5044696e-05\n",
            "Epoch 157. Train Loss: 1.4556693e-05, Test Loss : 8.386031e-05\n",
            "Epoch 158. Train Loss: 1.510774e-05, Test Loss : 0.00013801681\n",
            "Epoch 159. Train Loss: 1.473548e-05, Test Loss : 8.8264795e-05\n",
            "\u001b[92mâ˜‘\u001b[0m 2011-June-08 Wed = 06/08/2011, wednesday(06/08/2011, wednesday) 1, attention (21, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2005-December-01 Thu = 12/01/2005, thursday(12/01/2005, thursday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2003-September-21 Sun = 09/21/2003, sunday(09/21/2003, sunday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2008-February-01 Fri = 02/01/2008, friday(02/01/2008, friday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2004-December-04 Sat = 12/04/2004, saturday(12/04/2004, saturday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2008-September-04 Thu = 09/04/2008, thursday(09/04/2008, thursday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2009-May-06 Wed = 05/06/2009, wednesday(05/06/2009, wednesday) 1, attention (21, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2009-June-21 Sun = 06/21/2009, sunday(06/21/2009, sunday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2003-May-10 Sat = 05/10/2003, saturday(05/10/2003, saturday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2011-October-10 Mon = 10/10/2011, monday(10/10/2011, monday) 1, attention (18, 32)\n",
            "Epoch 160. Train Loss: 1.3463997e-05, Test Loss : 9.469219e-05\n",
            "Epoch 161. Train Loss: 1.3246782e-05, Test Loss : 8.983381e-05\n",
            "Epoch 162. Train Loss: 1.3246974e-05, Test Loss : 9.8532255e-05\n",
            "Epoch 163. Train Loss: 1.4407198e-05, Test Loss : 8.984644e-05\n",
            "Epoch 164. Train Loss: 1.3480267e-05, Test Loss : 9.0612724e-05\n",
            "Epoch 165. Train Loss: 1.3690504e-05, Test Loss : 8.586115e-05\n",
            "Epoch 166. Train Loss: 1.3290369e-05, Test Loss : 9.0246846e-05\n",
            "Epoch 167. Train Loss: 1.3289918e-05, Test Loss : 0.000104380175\n",
            "Epoch 168. Train Loss: 1.3496375e-05, Test Loss : 0.00011080145\n",
            "Epoch 169. Train Loss: 1.52285775e-05, Test Loss : 9.106218e-05\n",
            "\u001b[92mâ˜‘\u001b[0m 2010-March-20 Sat = 03/20/2010, saturday(03/20/2010, saturday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2009-February-25 Wed = 02/25/2009, wednesday(02/25/2009, wednesday) 1, attention (21, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2009-February-03 Tue = 02/03/2009, tuesday(02/03/2009, tuesday) 1, attention (19, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2006-October-14 Sat = 10/14/2006, saturday(10/14/2006, saturday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2003-April-28 Mon = 04/28/2003, monday(04/28/2003, monday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2007-February-14 Wed = 02/14/2007, wednesday(02/14/2007, wednesday) 1, attention (21, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2004-March-31 Wed = 03/31/2004, wednesday(03/31/2004, wednesday) 1, attention (21, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2010-February-21 Sun = 02/21/2010, sunday(02/21/2010, sunday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2008-April-03 Thu = 04/03/2008, thursday(04/03/2008, thursday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2007-December-11 Tue = 12/11/2007, tuesday(12/11/2007, tuesday) 1, attention (19, 32)\n",
            "Epoch 170. Train Loss: 1.3232737e-05, Test Loss : 9.8664306e-05\n",
            "Epoch 171. Train Loss: 1.3300357e-05, Test Loss : 0.00011013445\n",
            "Epoch 172. Train Loss: 1.284433e-05, Test Loss : 9.687693e-05\n",
            "Epoch 173. Train Loss: 1.285584e-05, Test Loss : 9.132506e-05\n",
            "Epoch 174. Train Loss: 1.4395045e-05, Test Loss : 9.0782334e-05\n",
            "Epoch 175. Train Loss: 1.3904111e-05, Test Loss : 0.00010371242\n",
            "Epoch 176. Train Loss: 1.3947057e-05, Test Loss : 9.923597e-05\n",
            "Epoch 177. Train Loss: 1.3678803e-05, Test Loss : 8.713781e-05\n",
            "Epoch 178. Train Loss: 1.6335269e-05, Test Loss : 0.00012903879\n",
            "Epoch 179. Train Loss: 1.2791554e-05, Test Loss : 0.000101499005\n",
            "\u001b[92mâ˜‘\u001b[0m 2012-June-08 Fri = 06/08/2012, friday(06/08/2012, friday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2004-July-06 Tue = 07/06/2004, tuesday(07/06/2004, tuesday) 1, attention (19, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2012-January-31 Tue = 01/31/2012, tuesday(01/31/2012, tuesday) 1, attention (19, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2009-February-13 Fri = 02/13/2009, friday(02/13/2009, friday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2010-January-03 Sun = 01/03/2010, sunday(01/03/2010, sunday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2002-July-25 Thu = 07/25/2002, thursday(07/25/2002, thursday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2005-October-31 Mon = 10/31/2005, monday(10/31/2005, monday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2005-September-23 Fri = 09/23/2005, friday(09/23/2005, friday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2010-February-25 Thu = 02/25/2010, thursday(02/25/2010, thursday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2005-March-26 Sat = 03/26/2005, saturday(03/26/2005, saturday) 1, attention (20, 32)\n",
            "Epoch 180. Train Loss: 1.3448722e-05, Test Loss : 9.571778e-05\n",
            "Epoch 181. Train Loss: 1.2504601e-05, Test Loss : 8.823863e-05\n",
            "Epoch 182. Train Loss: 1.3730889e-05, Test Loss : 0.00010023429\n",
            "Epoch 183. Train Loss: 1.3362152e-05, Test Loss : 0.00010255329\n",
            "Epoch 184. Train Loss: 1.2822032e-05, Test Loss : 8.997291e-05\n",
            "Epoch 185. Train Loss: 1.3262899e-05, Test Loss : 0.00012440186\n",
            "Epoch 186. Train Loss: 1.2881758e-05, Test Loss : 8.8495704e-05\n",
            "Epoch 187. Train Loss: 1.3427031e-05, Test Loss : 9.070224e-05\n",
            "Epoch 188. Train Loss: 1.2582717e-05, Test Loss : 8.95103e-05\n",
            "Epoch 189. Train Loss: 1.269365e-05, Test Loss : 9.282153e-05\n",
            "\u001b[92mâ˜‘\u001b[0m 2010-November-01 Mon = 11/01/2010, monday(11/01/2010, monday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2004-January-01 Thu = 01/01/2004, thursday(01/01/2004, thursday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2002-February-27 Wed = 02/27/2002, wednesday(02/27/2002, wednesday) 1, attention (21, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2012-March-21 Wed = 03/21/2012, wednesday(03/21/2012, wednesday) 1, attention (21, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2006-February-15 Wed = 02/15/2006, wednesday(02/15/2006, wednesday) 1, attention (21, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2008-February-18 Mon = 02/18/2008, monday(02/18/2008, monday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2007-October-20 Sat = 10/20/2007, saturday(10/20/2007, saturday) 1, attention (20, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2005-April-18 Mon = 04/18/2005, monday(04/18/2005, monday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2008-October-26 Sun = 10/26/2008, sunday(10/26/2008, sunday) 1, attention (18, 32)\n",
            "\u001b[92mâ˜‘\u001b[0m 2008-March-14 Fri = 03/14/2008, friday(03/14/2008, friday) 1, attention (18, 32)\n",
            "Epoch 190. Train Loss: 1.25680035e-05, Test Loss : 9.697443e-05\n",
            "Epoch 191. Train Loss: 1.3220404e-05, Test Loss : 9.843378e-05\n",
            "Epoch 192. Train Loss: 1.32321675e-05, Test Loss : 9.2429604e-05\n",
            "Epoch 193. Train Loss: 1.2780184e-05, Test Loss : 9.814531e-05\n",
            "Epoch 194. Train Loss: 1.2343471e-05, Test Loss : 9.249486e-05\n",
            "Epoch 195. Train Loss: 1.21159055e-05, Test Loss : 9.519849e-05\n",
            "Epoch 196. Train Loss: 1.25682745e-05, Test Loss : 9.648827e-05\n",
            "Epoch 197. Train Loss: 1.2423373e-05, Test Loss : 9.1965005e-05\n",
            "Epoch 198. Train Loss: 1.2919622e-05, Test Loss : 9.833226e-05\n",
            "Epoch 199. Train Loss: 1.2444178e-05, Test Loss : 9.5371535e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP4C1OtlKfMx"
      },
      "source": [
        "def plot_attention(net, data, ctx = mx.cpu()):\n",
        "    from matplotlib import pyplot as plt\n",
        "    import seaborn as sns\n",
        "    from sklearn.preprocessing import normalize\n",
        "    \n",
        "    sns.set()\n",
        "    p =[]\n",
        "    attn = []\n",
        "    for i, d  in enumerate(data):\n",
        "        _p, _attn = net.predict(d, char_indices, indices_char, input_digits = in_seq_len, lchars = len(chars))\n",
        "        p.append(_p.strip())\n",
        "        attn.append(_attn)\n",
        "\n",
        "    fig, axes = plt.subplots(np.int(np.ceil(len(data) / 1)), 1, sharex = False, sharey = False)\n",
        "    plt.subplots_adjust(hspace=1)\n",
        "\n",
        "    if len(data) > 1:\n",
        "        fig.set_size_inches(5, 40)\n",
        "    else:\n",
        "        fig.set_size_inches(10, 10)\n",
        "    plt.subplots_adjust(hspace=1)\n",
        "    \n",
        "    for i, (d, p, a) in enumerate(zip(data, p, attn)):\n",
        "        _col = list(d)\n",
        "        _idx = list(p)\n",
        "        #a = a[:len(p), :len(d)]\n",
        "        #a = normalize(a, axis = 0, norm = 'max')\n",
        "        #print(a)\n",
        "        _val = a[:len(p), :len(d)]\n",
        "        \n",
        "        print('input: {}, length: {}'.format(d,len(d)))\n",
        "        print('prediction: {}, length:{}'.format(p,len(p)))\n",
        "        print('attention shape= {}'.format(a.shape))\n",
        "        print('check attn = {}'.format(np.sum(a, axis = 0)))\n",
        "        print('val shape= {}'.format(_val.shape))\n",
        "        if len(data) > 1:\n",
        "            sns.heatmap(pd.DataFrame(_val, index = _idx, columns = _col), ax = axes.flat[i], cmap = 'RdYlGn', linewidths = .3)\n",
        "        else:\n",
        "            sns.heatmap(pd.DataFrame(_val, index = _idx, columns = _col), cmap = 'RdYlGn', linewidths = .3)\n",
        "        #axes.flat[i].set_title('Label: {}, Pred: {}'.format(_label[i], np.int(_pred[i])))\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v0FeFLnWGuh",
        "outputId": "3b92c937-ccb6-49c7-dfed-7f92c9966408",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(np.max(attn_1, axis = 0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97i3xc4QKfMy",
        "outputId": "d53a6d1f-bc0f-40f0-c830-4a838f059a03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        }
      },
      "source": [
        "example = [gen_date() for _ in range(1)]\n",
        "plot_attention(model, example)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input: 2005-September-01 Thu, length: 21\n",
            "prediction: 09/01/2005, thursday, length:20\n",
            "attention shape= (20, 32)\n",
            "check attn = [0.47629026 0.477359   0.53488195 1.1419923  0.5697325  0.4994407\n",
            " 0.5469731  0.48275506 0.48679817 0.5398182  1.0006263  1.0403169\n",
            " 0.8390795  0.7868108  0.92494714 0.8902244  0.99024117 0.751073\n",
            " 0.62510175 0.6042013  0.4845354  0.5081174  0.45892748 0.41344175\n",
            " 0.40533555 0.41758126 0.43533063 0.48209125 0.49592862 0.54426783\n",
            " 0.5633157  0.58246386]\n",
            "val shape= (20, 21)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAJHCAYAAAByw0fcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0u37NfLFOCNj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}