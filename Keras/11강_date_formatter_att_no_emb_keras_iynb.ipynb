{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11강-date_formatter_att_no_emb_keras.iynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/data-better/ASL/blob/master/11%EA%B0%95_date_formatter_att_no_emb_keras_iynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51-CVSwPsY0d"
      },
      "source": [
        "## Original Source:\n",
        "- https://www.tensorflow.org/beta/tutorials/text/nmt_with_attention\n",
        "\n",
        "### Date Format\n",
        "[2030-December-12 Thu] -> [12/12/2030, thursday]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KB9CqnWctXVE",
        "outputId": "3d7e0064-a6f4-4971-9639-5984e4fae350",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import datetime\n",
        "from datetime import datetime\n",
        "\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0ktwcLVtx-F"
      },
      "source": [
        "# Data Generation\n",
        "## functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4z5hM8ttZZe"
      },
      "source": [
        "def padding(chars, maxlen):\n",
        "    if len(chars) < maxlen:\n",
        "        return chars + '_' * (maxlen - len(chars))\n",
        "    else:\n",
        "        return chars[:maxlen]\n",
        "\n",
        "def gen_date():\n",
        "    rnd = int(np.random.uniform(low = 1e9, high = 2e9)) # 2001.9.9 ~ 2033.5.18 \n",
        "    timestamp = datetime.fromtimestamp(rnd)\n",
        "    return str(timestamp.strftime('%Y-%B-%d %a')) # '%Y-%B-%d %H:%M:%S' ex) 2030-December-12 Thu\n",
        "\n",
        "    \n",
        "def format_date(x):\n",
        "    return str(datetime.strptime(x, '%Y-%B-%d %a').strftime('%m/%d/%Y, %A')).lower() #'%H%M%S:%Y%m%d' ex) 12/12/2030, thursday\n",
        "\n",
        "  \n",
        "def gen_test(N):\n",
        "    q = []\n",
        "    y = []\n",
        "    \n",
        "    for i in range(N):\n",
        "        question = gen_date()\n",
        "        answer_y = format_date(question)\n",
        "        q.append(question)\n",
        "        y.append(answer_y)\n",
        "    return(q,y)\n",
        "  \n",
        "class colors:\n",
        "    ok = '\\033[92m'\n",
        "    fail = '\\033[91m'\n",
        "    close = '\\033[0m'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKe5I4PNvlex"
      },
      "source": [
        "## Generate data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2Scsxh4j1f3"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXpxhrtXvkaL"
      },
      "source": [
        "N = 5000\n",
        "N_train = int(N * .9)\n",
        "N_validation = N - N_train\n",
        "\n",
        "in_seq_len = 32\n",
        "out_seq_len = 32\n",
        "\n",
        "added = set()\n",
        "questions = []\n",
        "answers = []\n",
        "\n",
        "\n",
        "while len(questions) < N:\n",
        "    a = gen_date()\n",
        "    if a in added:\n",
        "        continue\n",
        "    question = '[{}]'.format(a)\n",
        "    question = padding(question, in_seq_len)\n",
        "    answer = '[' + str(format_date(a)) + ']'\n",
        "    answer = padding(answer, out_seq_len)\n",
        "\n",
        "    added.add(a)\n",
        "    questions.append(question)\n",
        "    answers.append(answer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWuI-KXDt28l"
      },
      "source": [
        "### Check Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iod24gRcvC3J",
        "outputId": "9168b6b8-c370-4dc2-801f-40a20633fcd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(questions[:3])\n",
        "print(answers[:3])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[2012-April-04 Wed]_____________', '[2003-November-27 Thu]__________', '[2011-April-02 Sat]_____________']\n",
            "['[04/04/2012, wednesday]_________', '[11/27/2003, thursday]__________', '[04/02/2011, saturday]__________']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqGiJo4et5jf"
      },
      "source": [
        "## One-hot encoding\n",
        "\n",
        "* vocab_size = 48"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daM8UPF6vlkm"
      },
      "source": [
        "chars = list(set(''.join(questions)))\n",
        "chars.extend(['[', ']']) # Start and End of Expression\n",
        "chars.extend(list(set(''.join(answers))))\n",
        "chars = np.sort(list(set(chars)))\n",
        "\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "X = np.zeros((len(questions), in_seq_len, len(chars)), dtype='float32')\n",
        "Y = np.zeros((len(questions), out_seq_len, len(chars)), dtype='float32')\n",
        "\n",
        "for i in range(N):\n",
        "    for t, char in enumerate(questions[i]):\n",
        "        X[i, t, char_indices[char]] = 1\n",
        "    for t, char in enumerate(answers[i]):\n",
        "        Y[i, t, char_indices[char]] = 1\n",
        "       \n",
        "X_train, X_validation, Y_train, Y_validation = \\\n",
        "    train_test_split(X, Y, train_size=N_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eaUfjYHOm3l"
      },
      "source": [
        "### train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7f59X85H2aZ",
        "outputId": "d769935d-3f82-42ec-ff97-7bab1e1b19f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "print(X_train[0,:,:].shape)\n",
        "print(question[0])\n",
        "print(X_train[0,0,:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 48)\n",
            "[\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK_DNOPYuJIn"
      },
      "source": [
        "### char_indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDx3m9-xuOud",
        "outputId": "c4345adf-f694-43a9-a678-5732fe6628c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(char_indices)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{' ': 0, ',': 1, '-': 2, '/': 3, '0': 4, '1': 5, '2': 6, '3': 7, '4': 8, '5': 9, '6': 10, '7': 11, '8': 12, '9': 13, 'A': 14, 'D': 15, 'F': 16, 'J': 17, 'M': 18, 'N': 19, 'O': 20, 'S': 21, 'T': 22, 'W': 23, '[': 24, ']': 25, '_': 26, 'a': 27, 'b': 28, 'c': 29, 'd': 30, 'e': 31, 'f': 32, 'g': 33, 'h': 34, 'i': 35, 'l': 36, 'm': 37, 'n': 38, 'o': 39, 'p': 40, 'r': 41, 's': 42, 't': 43, 'u': 44, 'v': 45, 'w': 46, 'y': 47}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHd7ooDnySEW",
        "outputId": "382d58c9-25e5-4bef-b733-c204049c3619",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.test.is_gpu_available()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vud6SUQtvTc9"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sayB_nSx4rnq"
      },
      "source": [
        "#### Glorot Uniform Initializer\n",
        "\n",
        "  - $ u \\sim \\left[-\\sqrt{\\frac 6{d_{in} + d_{out}}}, \\sqrt{\\frac 6{d_{in} + d_{out}}}\\right]$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SAFNb14zYoQ"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, enc_units, batch_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.enc_units = enc_units\n",
        "    self.gru = tf.keras.layers.CuDNNGRU(self.enc_units,\n",
        "                      return_sequences = True,\n",
        "                      return_state = True,\n",
        "                      recurrent_initializer = 'glorot_uniform' # xavier init\n",
        "                      )\n",
        "    \n",
        "  def call(self, x, hidden): \n",
        "    # x: Batch size * in_seq_len * inp_vocab_size\n",
        "    # hidden: batch_size * enc_units\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    # output: batch_size * in_seq_len * enc_units\n",
        "    # state: batch * enc_units\n",
        "    return output, state\n",
        "  \n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_size, self.enc_units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAxb0DSgLN7I"
      },
      "source": [
        "### Bahdanau Attention\n",
        "\n",
        "  - Bahdanau alignment score (additive score) = $ v^T \\tanh (W_1 h_t + W_2 \\bar h_s)$\n",
        "\n",
        "  - value : $\\bar h_s, s = 1, \\ldots, $ input sequence length\n",
        "\n",
        "  - query: $h_t$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fekg4Sosx5Bi"
      },
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query shape: (batch_size, dec_unit)\n",
        "    # values: (batch_size, max_length, enc_unit)\n",
        "    \n",
        "    # hidden_with_time_axis shape: (batch_size, 1, dec_unit)\n",
        "    hidden_with_time_axis = tf.expand_dim(query, 1)\n",
        "\n",
        "    \n",
        "    # 마지막 dim을 input dim으로 간주하고 나머지 shape는 모두 유지: context_unit -> 1\n",
        "    score = self.V(tf.nn.tanh(self.W1(hidden_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    \n",
        "    # self.W1(hidden_with_time_axis): batch_size * 1 * context_unit\n",
        "    # self.W2(values): batch_size * in_seq_len * context_unit\n",
        "    # score shape: batch_size * in_seq_len * 1\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXZv1DiTD74q"
      },
      "source": [
        "- 1 step만 가정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnMbwilT3Uqh"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  # vocab_size: 48\n",
        "  # dec_unit: decoder hidden state\n",
        "  def __init__(self, vocab_size, dec_units, context_units, batch_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.dec_units = dec_units # decoder hidden state 크기\n",
        "    self.context_units = context_units\n",
        "    self.gru = tf.keras.layers.CuDNNGRU(dec_units,\n",
        "                      return_sequences = True,\n",
        "                      return_state = True,\n",
        "                      recurrent_initializer = 'glorot_uniform' # xavier init.\n",
        "                      )\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "    self.attention = BahdanauAttention(self.context_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # x: batch_size * 1 * vocab_size\n",
        "    # hidden: batch_size * dec_unit\n",
        "    # enc_output: batch_size * in_seq_len * enc_unit\n",
        "\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "    # context_vector: batch_size * enc_unit\n",
        "    # attention_weights: batch_size * in_seq_len * 1\n",
        "    \n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis = -1)\n",
        "    output, state = self.gru(x)\n",
        "    #output: batch_size * 1* dec_unit (1 time step)\n",
        "    output = tf.reshape(output, (-1, output.shape[2])) # 가운데 dimension 삭제\n",
        "    #output: batch_size * dec_unit\n",
        "    \n",
        "    x = self.fc(output)\n",
        "    # x: batch_size * vocab_size\n",
        "    \n",
        "    return x, state, attention_weights\n",
        "  \n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_size, self.dec_units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOoGfDNIBiKG"
      },
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zepMaq0ZkUX3"
      },
      "source": [
        "def evaluate(sentence, encoder, decoder, char_indices, indices_char, max_len_inp, max_len_targ, enc_units, dec_units, context_units):\n",
        "  attention_plot = np.zeros((max_len_targ, max_len_inp))\n",
        "  inp = np.zeros((max_len_inp, len(char_indices)), dtype=np.integer)\n",
        "  for t, char in enumerate(sentence):\n",
        "    inp[t, char_indices[char]] = 1 \n",
        "  inp = tf.convert_to_tensor(inp, dtype = tf.float32)\n",
        "  inp = tf.expand_dims(inp, axis = 0)\n",
        "  result = ''\n",
        "  hidden = [tf.zeros((1, enc_units))]\n",
        "  enc_out, enc_hidden = encoder(inp, hidden)\n",
        "  dec_hidden = enc_hidden # Initialize with encoder hidden\n",
        "  dec_inp = np.zeros(len(char_indices))\n",
        "  dec_inp[char_indices['[']] = 1\n",
        "  dec_inp = tf.convert_to_tensor(dec_inp, tf.float32)\n",
        "  dec_inp = tf.expand_dims(dec_inp, 0)\n",
        "  dec_inp = tf.expand_dims(dec_inp, 0)\n",
        "  \n",
        "  for t in range(max_len_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_inp, dec_hidden, enc_out)\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "    \n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "    \n",
        "    result += indices_char[predicted_id]\n",
        "    if indices_char[predicted_id] == ']':\n",
        "      return result, sentence, attention_plot\n",
        "    \n",
        "    dec_inp = np.zeros(len(char_indices))\n",
        "    dec_inp[predicted_id] = 1\n",
        "    dec_inp = tf.convert_to_tensor(dec_inp, dtype = tf.float32)\n",
        "    dec_inp = tf.expand_dims(dec_inp, 0)\n",
        "    dec_inp = tf.expand_dims(dec_inp, 0)\n",
        "    \n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEVb_1NZN7SV"
      },
      "source": [
        "def evaluate_some(n, encoder, decoder, char_indices, indices_char, max_len_inp, max_len_targ, enc_units, dec_units, context_units):\n",
        "  q, y = gen_test(n)\n",
        "  for i in range(n):\n",
        "    print(q[i])\n",
        "    p, _, _ = evaluate(q[i], encoder, decoder, char_indices, indices_char, max_len_inp, max_len_targ, enc_units, dec_units, context_units)\n",
        "    p = p.strip().replace(']', '')\n",
        "    iscorr = 1 if p == y[i] else 0\n",
        "    if iscorr == 1:\n",
        "      print(colors.ok + '☑' + colors.close, end=' ')\n",
        "    else:\n",
        "      print(colors.fail + '☒' + colors.close, end=' ')\n",
        "    print(\"{} = {}({})\".format(q[i], p, y[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aviB-1ybofa3"
      },
      "source": [
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  out_seq_len, in_seq_len = 32, 32\n",
        "  fig = plt.figure(figsize = (10, 10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap = 'viridis')\n",
        "  fontdict = {'fontsize': 12}\n",
        "  ax.set_xticks([x for x in list(range(in_seq_len))])\n",
        "  ax.set_yticks([x for x in list(range(out_seq_len))])\n",
        "  ax.set_xticklabels(sentence, fontdict = fontdict)\n",
        "  ax.set_yticklabels(predicted_sentence, fontdict = fontdict)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym2maA3lBpmS"
      },
      "source": [
        "# Define parameters and model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27LnC04TLp_b"
      },
      "source": [
        "BUFFER_SIZE = len(X_train)\n",
        "BATCH_SIZE = 64\n",
        "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
        "enc_units, dec_units = 128, 128  # gru hidden unit\n",
        "context_units = 64\n",
        "vocab_inp_size = len(chars)\n",
        "vocab_tar_size = len(chars)\n",
        "\n",
        "encoder = Encoder(enc_units, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_tar_size, dec_units, context_units, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8sM_6_7Mczn"
      },
      "source": [
        "optimizer = tf.train.AdamOptimizer()\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  loss_ = tf.nn.softmax_cross_entropy_with_logits(labels = real, logits = pred)\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqtrr69JNL9Q"
      },
      "source": [
        "import os\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
        "checkpoint = tf.train.Checkpoint(optimizer = optimizer,\n",
        "                                 encoder = encoder,\n",
        "                                 decoder = decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHM56mwUBtM9"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLppBoUKN3yq",
        "outputId": "67760228-de2d-4c24-a78d-53b5ee120456",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import time\n",
        "\n",
        "EPOCHS = 100\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "  \n",
        "  for batch in range(N_BATCH):\n",
        "    loss = 0\n",
        "    idx = np.arange(batch * BATCH_SIZE, (batch + 1) * BATCH_SIZE)\n",
        "    inp = X_train[idx, :]\n",
        "    targ = Y_train[idx, :]\n",
        "    with tf.GradientTape() as tape:\n",
        "      enc_output, enc_hidden = encoder(inp, hidden)\n",
        "      dec_hidden = enc_hidden\n",
        "\n",
        "\n",
        "      inp = np.zeros((len(chars)), dtype='float32')\n",
        "      inp[char_indices['[']] = 1\n",
        "      dec_input = tf.expand_dims([inp] * BATCH_SIZE, 1) # batch_size * 1 * vocab_size\n",
        "      \n",
        "      \n",
        "      for t in range(1, targ.shape[1]):\n",
        "        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "        real = tf.cast(tf.squeeze(targ[:, t, :]), tf.float32)\n",
        "        pred = tf.cast(predictions, tf.float32)\n",
        "        loss += loss_function(real, pred)\n",
        "        dec_input = tf.expand_dims(targ[:, t], 1)# Teacher forcing\n",
        "      \n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "    total_loss += batch_loss\n",
        "      \n",
        "    variables = encoder.variables + decoder.variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "      \n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "      \n",
        "\n",
        "  if (epoch + 1) % 1 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    evaluate_some(10, encoder, decoder, char_indices, indices_char, in_seq_len, out_seq_len, enc_units, dec_units, context_units)\n",
        "  print('Epoch {} Loss {:.6f}, Elapsed_time ={:.2f} sec'.format(epoch + 1, \n",
        "                                                                total_loss / N_BATCH, \n",
        "                                                                time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0812 06:12:55.692327 140222269294464 deprecation.py:323] From <ipython-input-16-9af9741125da>:9: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2007-October-18 Thu\n",
            "\u001b[91m☒\u001b[0m 2007-October-18 Thu = 0_______________________________(10/18/2007, thursday)\n",
            "2012-April-14 Sat\n",
            "\u001b[91m☒\u001b[0m 2012-April-14 Sat = 0_______________________________(04/14/2012, saturday)\n",
            "2023-August-24 Thu\n",
            "\u001b[91m☒\u001b[0m 2023-August-24 Thu = 0_______________________________(08/24/2023, thursday)\n",
            "2028-November-07 Tue\n",
            "\u001b[91m☒\u001b[0m 2028-November-07 Tue = 0_______________________________(11/07/2028, tuesday)\n",
            "2012-August-02 Thu\n",
            "\u001b[91m☒\u001b[0m 2012-August-02 Thu = 0_______________________________(08/02/2012, thursday)\n",
            "2023-December-04 Mon\n",
            "\u001b[91m☒\u001b[0m 2023-December-04 Mon = 0_______________________________(12/04/2023, monday)\n",
            "2021-November-07 Sun\n",
            "\u001b[91m☒\u001b[0m 2021-November-07 Sun = 0_______________________________(11/07/2021, sunday)\n",
            "2027-December-27 Mon\n",
            "\u001b[91m☒\u001b[0m 2027-December-27 Mon = 0_______________________________(12/27/2027, monday)\n",
            "2027-September-11 Sat\n",
            "\u001b[91m☒\u001b[0m 2027-September-11 Sat = 0_______________________________(09/11/2027, saturday)\n",
            "2029-January-06 Sat\n",
            "\u001b[91m☒\u001b[0m 2029-January-06 Sat = 0_______________________________(01/06/2029, saturday)\n",
            "Epoch 1 Loss 2.635025, Elapsed_time =121.23 sec\n",
            "2012-December-01 Sat\n",
            "\u001b[91m☒\u001b[0m 2012-December-01 Sat = 01/201/201/201/201/201/201/201/2(12/01/2012, saturday)\n",
            "2002-May-25 Sat\n",
            "\u001b[91m☒\u001b[0m 2002-May-25 Sat = 01/201/201/201/201/201/201/201/2(05/25/2002, saturday)\n",
            "2008-November-19 Wed\n",
            "\u001b[91m☒\u001b[0m 2008-November-19 Wed = 01/201/201/201/201/201/201/201/2(11/19/2008, wednesday)\n",
            "2006-June-04 Sun\n",
            "\u001b[91m☒\u001b[0m 2006-June-04 Sun = 01/201/201/201/201/201/201/201/2(06/04/2006, sunday)\n",
            "2018-September-25 Tue\n",
            "\u001b[91m☒\u001b[0m 2018-September-25 Tue = 01/201/201/201/201/201/201/201/2(09/25/2018, tuesday)\n",
            "2020-June-16 Tue\n",
            "\u001b[91m☒\u001b[0m 2020-June-16 Tue = 01/201/201/201/201/201/201/201/2(06/16/2020, tuesday)\n",
            "2012-February-22 Wed\n",
            "\u001b[91m☒\u001b[0m 2012-February-22 Wed = 01/201/201/201/201/201/201/201/2(02/22/2012, wednesday)\n",
            "2014-January-05 Sun\n",
            "\u001b[91m☒\u001b[0m 2014-January-05 Sun = 01/201/201/201/201/201/201/201/2(01/05/2014, sunday)\n",
            "2031-May-30 Fri\n",
            "\u001b[91m☒\u001b[0m 2031-May-30 Fri = 01/201/201/201/201/201/201/201/2(05/30/2031, friday)\n",
            "2007-October-27 Sat\n",
            "\u001b[91m☒\u001b[0m 2007-October-27 Sat = 01/201/201/201/201/201/201/201/2(10/27/2007, saturday)\n",
            "Epoch 2 Loss 1.590442, Elapsed_time =118.66 sec\n",
            "2030-March-10 Sun\n",
            "\u001b[91m☒\u001b[0m 2030-March-10 Sun = 01/201/201/201/201/201/201/201/2(03/10/2030, sunday)\n",
            "2009-March-28 Sat\n",
            "\u001b[91m☒\u001b[0m 2009-March-28 Sat = 01/201/201/201/201/201/201/201/2(03/28/2009, saturday)\n",
            "2014-September-18 Thu\n",
            "\u001b[91m☒\u001b[0m 2014-September-18 Thu = 01/201/201/201/201/201/201/201/2(09/18/2014, thursday)\n",
            "2012-February-15 Wed\n",
            "\u001b[91m☒\u001b[0m 2012-February-15 Wed = 01/201/201/201/201/201/201/201/2(02/15/2012, wednesday)\n",
            "2020-December-16 Wed\n",
            "\u001b[91m☒\u001b[0m 2020-December-16 Wed = 01/201/201/201/201/201/201/201/2(12/16/2020, wednesday)\n",
            "2023-November-22 Wed\n",
            "\u001b[91m☒\u001b[0m 2023-November-22 Wed = 01/201/201/201/201/201/201/201/2(11/22/2023, wednesday)\n",
            "2021-May-06 Thu\n",
            "\u001b[91m☒\u001b[0m 2021-May-06 Thu = 01/201/201/201/201/201/201/201/2(05/06/2021, thursday)\n",
            "2011-September-03 Sat\n",
            "\u001b[91m☒\u001b[0m 2011-September-03 Sat = 01/201/201/201/201/201/201/201/2(09/03/2011, saturday)\n",
            "2007-October-03 Wed\n",
            "\u001b[91m☒\u001b[0m 2007-October-03 Wed = 01/201/201/201/201/201/201/201/2(10/03/2007, wednesday)\n",
            "2030-November-15 Fri\n",
            "\u001b[91m☒\u001b[0m 2030-November-15 Fri = 01/201/201/201/201/201/201/201/2(11/15/2030, friday)\n",
            "Epoch 3 Loss 1.082432, Elapsed_time =117.90 sec\n",
            "2026-January-03 Sat\n",
            "\u001b[91m☒\u001b[0m 2026-January-03 Sat = 02/202/202/202/202/202/202/202/2(01/03/2026, saturday)\n",
            "2019-May-28 Tue\n",
            "\u001b[91m☒\u001b[0m 2019-May-28 Tue = 02/202/202/202/202/202/202/202/2(05/28/2019, tuesday)\n",
            "2008-September-18 Thu\n",
            "\u001b[91m☒\u001b[0m 2008-September-18 Thu = 02/201/201/201/201/201/201/201/2(09/18/2008, thursday)\n",
            "2033-March-22 Tue\n",
            "\u001b[91m☒\u001b[0m 2033-March-22 Tue = 02/202/202/202/202/202/202/202/2(03/22/2033, tuesday)\n",
            "2032-February-26 Thu\n",
            "\u001b[91m☒\u001b[0m 2032-February-26 Thu = 02/202/202/202/202/202/202/202/2(02/26/2032, thursday)\n",
            "2025-March-24 Mon\n",
            "\u001b[91m☒\u001b[0m 2025-March-24 Mon = 02/202/202/202/202/202/202/202/2(03/24/2025, monday)\n",
            "2001-December-01 Sat\n",
            "\u001b[91m☒\u001b[0m 2001-December-01 Sat = 01/201/201/201/201/201/201/201/2(12/01/2001, saturday)\n",
            "2005-February-20 Sun\n",
            "\u001b[91m☒\u001b[0m 2005-February-20 Sun = 02/201/201/201/201/201/201/201/2(02/20/2005, sunday)\n",
            "2010-September-19 Sun\n",
            "\u001b[91m☒\u001b[0m 2010-September-19 Sun = 01/201/201/201/201/201/201/201/2(09/19/2010, sunday)\n",
            "2003-August-10 Sun\n",
            "\u001b[91m☒\u001b[0m 2003-August-10 Sun = 02/202/202/202/202/202/202/202/2(08/10/2003, sunday)\n",
            "Epoch 4 Loss 0.745526, Elapsed_time =119.35 sec\n",
            "2002-July-30 Tue\n",
            "\u001b[91m☒\u001b[0m 2002-July-30 Tue = 02/202/202/202/202/202/202/202/2(07/30/2002, tuesday)\n",
            "2001-September-29 Sat\n",
            "\u001b[91m☒\u001b[0m 2001-September-29 Sat = 02/202/202/202/202/202/202/202/2(09/29/2001, saturday)\n",
            "2025-May-20 Tue\n",
            "\u001b[91m☒\u001b[0m 2025-May-20 Tue = 02/202/202/202/202/202/202/202/2(05/20/2025, tuesday)\n",
            "2008-April-18 Fri\n",
            "\u001b[91m☒\u001b[0m 2008-April-18 Fri = 02/202/202/202/202/202/202/202/2(04/18/2008, friday)\n",
            "2011-August-15 Mon\n",
            "\u001b[91m☒\u001b[0m 2011-August-15 Mon = 02/202/202/202/202/202/202/202/2(08/15/2011, monday)\n",
            "2031-February-03 Mon\n",
            "\u001b[91m☒\u001b[0m 2031-February-03 Mon = 01/202/202/202/202/202/202/202/2(02/03/2031, monday)\n",
            "2027-August-14 Sat\n",
            "\u001b[91m☒\u001b[0m 2027-August-14 Sat = 02/202/202/202/202/202/202/202/2(08/14/2027, saturday)\n",
            "2027-March-14 Sun\n",
            "\u001b[91m☒\u001b[0m 2027-March-14 Sun = 02/202/202/202/202/202/202/202/2(03/14/2027, sunday)\n",
            "2012-January-22 Sun\n",
            "\u001b[91m☒\u001b[0m 2012-January-22 Sun = 01/202/202/202/202/202/202/202/2(01/22/2012, sunday)\n",
            "2006-November-27 Mon\n",
            "\u001b[91m☒\u001b[0m 2006-November-27 Mon = 01/202/202/202/202/202/202/202/2(11/27/2006, monday)\n",
            "Epoch 5 Loss 0.633547, Elapsed_time =119.47 sec\n",
            "2018-January-17 Wed\n",
            "\u001b[91m☒\u001b[0m 2018-January-17 Wed = 01/202/201/202/201/202/201/202/2(01/17/2018, wednesday)\n",
            "2013-November-22 Fri\n",
            "\u001b[91m☒\u001b[0m 2013-November-22 Fri = 01/201/201/201/201/201/201/201/2(11/22/2013, friday)\n",
            "2010-January-10 Sun\n",
            "\u001b[91m☒\u001b[0m 2010-January-10 Sun = 01/201/201/201/201/201/201/201/2(01/10/2010, sunday)\n",
            "2007-February-11 Sun\n",
            "\u001b[91m☒\u001b[0m 2007-February-11 Sun = 02/202/202/202/202/202/202/202/2(02/11/2007, sunday)\n",
            "2011-April-21 Thu\n",
            "\u001b[91m☒\u001b[0m 2011-April-21 Thu = 02/201/202/201/202/201/202/201/2(04/21/2011, thursday)\n",
            "2024-October-09 Wed\n",
            "\u001b[91m☒\u001b[0m 2024-October-09 Wed = 01/202/202/202/202/202/202/202/2(10/09/2024, wednesday)\n",
            "2010-February-08 Mon\n",
            "\u001b[91m☒\u001b[0m 2010-February-08 Mon = 01/202/202/202/202/202/202/202/2(02/08/2010, monday)\n",
            "2030-October-01 Tue\n",
            "\u001b[91m☒\u001b[0m 2030-October-01 Tue = 01/202/201/201/201/201/201/201/2(10/01/2030, tuesday)\n",
            "2019-June-20 Thu\n",
            "\u001b[91m☒\u001b[0m 2019-June-20 Thu = 02/202/202/202/202/202/202/202/2(06/20/2019, thursday)\n",
            "2020-May-15 Fri\n",
            "\u001b[91m☒\u001b[0m 2020-May-15 Fri = 02/201/202/202/202/202/202/202/2(05/15/2020, friday)\n",
            "Epoch 6 Loss 0.554062, Elapsed_time =119.05 sec\n",
            "2020-June-15 Mon\n",
            "\u001b[91m☒\u001b[0m 2020-June-15 Mon = 02/202/202/202/202/202/202/202/2(06/15/2020, monday)\n",
            "2007-November-15 Thu\n",
            "\u001b[91m☒\u001b[0m 2007-November-15 Thu = 01/202/202/202/202/202/202/202/2(11/15/2007, thursday)\n",
            "2019-April-14 Sun\n",
            "\u001b[91m☒\u001b[0m 2019-April-14 Sun = 02/202/202/202/202/202/202/202/2(04/14/2019, sunday)\n",
            "2025-April-05 Sat\n",
            "\u001b[91m☒\u001b[0m 2025-April-05 Sat = 02/202/202/202/202/202/202/202/2(04/05/2025, saturday)\n",
            "2008-July-25 Fri\n",
            "\u001b[91m☒\u001b[0m 2008-July-25 Fri = 02/202/202/202/202/202/202/202/2(07/25/2008, friday)\n",
            "2029-October-20 Sat\n",
            "\u001b[91m☒\u001b[0m 2029-October-20 Sat = 02/202/202/202/202/202/202/202/2(10/20/2029, saturday)\n",
            "2010-November-06 Sat\n",
            "\u001b[91m☒\u001b[0m 2010-November-06 Sat = 01/202/202/202/202/202/202/202/2(11/06/2010, saturday)\n",
            "2032-April-03 Sat\n",
            "\u001b[91m☒\u001b[0m 2032-April-03 Sat = 02/202/202/202/202/202/202/202/2(04/03/2032, saturday)\n",
            "2027-May-13 Thu\n",
            "\u001b[91m☒\u001b[0m 2027-May-13 Thu = 02/202/202/202/202/202/202/202/2(05/13/2027, thursday)\n",
            "2015-September-11 Fri\n",
            "\u001b[91m☒\u001b[0m 2015-September-11 Fri = 02/202/202/202/202/202/202/202/2(09/11/2015, friday)\n",
            "Epoch 7 Loss 0.501192, Elapsed_time =118.65 sec\n",
            "2019-December-28 Sat\n",
            "\u001b[91m☒\u001b[0m 2019-December-28 Sat = 01/12/202/12/202/12/202/12/202/1(12/28/2019, saturday)\n",
            "2012-August-19 Sun\n",
            "\u001b[91m☒\u001b[0m 2012-August-19 Sun = 006, saturday(08/19/2012, sunday)\n",
            "2007-August-26 Sun\n",
            "\u001b[91m☒\u001b[0m 2007-August-26 Sun = 007, saturday(08/26/2007, sunday)\n",
            "2030-December-01 Sun\n",
            "\u001b[91m☒\u001b[0m 2030-December-01 Sun = 01/12/202/12/202/12/202/12/202/1(12/01/2030, sunday)\n",
            "2022-September-05 Mon\n",
            "\u001b[91m☒\u001b[0m 2022-September-05 Mon = 01/12/202/12/202/12/202/12/202/1(09/05/2022, monday)\n",
            "2026-January-25 Sun\n",
            "\u001b[91m☒\u001b[0m 2026-January-25 Sun = 01/12/202/12/202/12/202/12/202/1(01/25/2026, sunday)\n",
            "2032-April-13 Tue\n",
            "\u001b[91m☒\u001b[0m 2032-April-13 Tue = 004, saturday(04/13/2032, tuesday)\n",
            "2012-August-04 Sat\n",
            "\u001b[91m☒\u001b[0m 2012-August-04 Sat = 006, saturday(08/04/2012, saturday)\n",
            "2015-December-11 Fri\n",
            "\u001b[91m☒\u001b[0m 2015-December-11 Fri = 01/12/202/12/202/12/202/12/202/1(12/11/2015, friday)\n",
            "2033-January-10 Mon\n",
            "\u001b[91m☒\u001b[0m 2033-January-10 Mon = 01/202/202/202/202/202/202/202/2(01/10/2033, monday)\n",
            "Epoch 8 Loss 0.454459, Elapsed_time =117.87 sec\n",
            "2016-August-13 Sat\n",
            "\u001b[91m☒\u001b[0m 2016-August-13 Sat = 04/24/2004, saturday(08/13/2016, saturday)\n",
            "2018-January-04 Thu\n",
            "\u001b[91m☒\u001b[0m 2018-January-04 Thu = 02/12/201/202/202/12/201/202/202(01/04/2018, thursday)\n",
            "2030-March-22 Fri\n",
            "\u001b[91m☒\u001b[0m 2030-March-22 Fri = 02/12/202/20/201/20/201/20/201/2(03/22/2030, friday)\n",
            "2015-July-15 Wed\n",
            "\u001b[91m☒\u001b[0m 2015-July-15 Wed = 04/24/2006, saturday(07/15/2015, wednesday)\n",
            "2018-June-04 Mon\n",
            "\u001b[91m☒\u001b[0m 2018-June-04 Mon = 04/24/201/20/201/20/201/20/201/2(06/04/2018, monday)\n",
            "2001-November-12 Mon\n",
            "\u001b[91m☒\u001b[0m 2001-November-12 Mon = 11/20/201/11/201/11/201/11/201/1(11/12/2001, monday)\n",
            "2024-January-18 Thu\n",
            "\u001b[91m☒\u001b[0m 2024-January-18 Thu = 02/12/202/12/202/12/201/20/201/2(01/18/2024, thursday)\n",
            "2027-April-05 Mon\n",
            "\u001b[91m☒\u001b[0m 2027-April-05 Mon = 04/04/2024, saturday(04/05/2027, monday)\n",
            "2020-February-20 Thu\n",
            "\u001b[91m☒\u001b[0m 2020-February-20 Thu = 02/12/202/20/201/20/201/20/201/2(02/20/2020, thursday)\n",
            "2021-August-31 Tue\n",
            "\u001b[91m☒\u001b[0m 2021-August-31 Tue = 04/24/2006, saturday(08/31/2021, tuesday)\n",
            "Epoch 9 Loss 0.425061, Elapsed_time =124.13 sec\n",
            "2030-August-03 Sat\n",
            "\u001b[91m☒\u001b[0m 2030-August-03 Sat = 04/04/2004, saturday(08/03/2030, saturday)\n",
            "2011-September-18 Sun\n",
            "\u001b[91m☒\u001b[0m 2011-September-18 Sun = 02/20/2012, tuesday(09/18/2011, sunday)\n",
            "2019-November-28 Thu\n",
            "\u001b[91m☒\u001b[0m 2019-November-28 Thu = 11/11/2012, tuesday(11/28/2019, thursday)\n",
            "2005-May-11 Wed\n",
            "\u001b[91m☒\u001b[0m 2005-May-11 Wed = 04/04/2007, thursday(05/11/2005, wednesday)\n",
            "2006-March-28 Tue\n",
            "\u001b[91m☒\u001b[0m 2006-March-28 Tue = 04/20/2006, saturday(03/28/2006, tuesday)\n",
            "2003-May-06 Tue\n",
            "\u001b[91m☒\u001b[0m 2003-May-06 Tue = 04/04/2006, saturday(05/06/2003, tuesday)\n",
            "2030-April-14 Sun\n",
            "\u001b[91m☒\u001b[0m 2030-April-14 Sun = 04/24/2004, saturday(04/14/2030, sunday)\n",
            "2023-August-12 Sat\n",
            "\u001b[91m☒\u001b[0m 2023-August-12 Sat = 04/24/202/04/202/04/202/04/202/0(08/12/2023, saturday)\n",
            "2023-December-29 Fri\n",
            "\u001b[91m☒\u001b[0m 2023-December-29 Fri = 12/12/2022, tuesday(12/29/2023, friday)\n",
            "2009-April-23 Thu\n",
            "\u001b[91m☒\u001b[0m 2009-April-23 Thu = 04/20/2004, saturday(04/23/2009, thursday)\n",
            "Epoch 10 Loss 0.374141, Elapsed_time =99.45 sec\n",
            "2010-March-01 Mon\n",
            "\u001b[91m☒\u001b[0m 2010-March-01 Mon = 02/2020, tuesday(03/01/2010, monday)\n",
            "2022-August-19 Fri\n",
            "\u001b[91m☒\u001b[0m 2022-August-19 Fri = 04/202/202/202/202/202/202/202/2(08/19/2022, friday)\n",
            "2019-July-12 Fri\n",
            "\u001b[91m☒\u001b[0m 2019-July-12 Fri = 04/2020, tuesday(07/12/2019, friday)\n",
            "2023-August-11 Fri\n",
            "\u001b[91m☒\u001b[0m 2023-August-11 Fri = 04/2020, thursday(08/11/2023, friday)\n",
            "2022-September-11 Sun\n",
            "\u001b[91m☒\u001b[0m 2022-September-11 Sun = 02/2020, tuesday(09/11/2022, sunday)\n",
            "2011-September-30 Fri\n",
            "\u001b[91m☒\u001b[0m 2011-September-30 Fri = 02/2020, tuesday(09/30/2011, friday)\n",
            "2003-October-25 Sat\n",
            "\u001b[91m☒\u001b[0m 2003-October-25 Sat = 01/11/201/20/201/20/201/20/201/2(10/25/2003, saturday)\n",
            "2024-October-17 Thu\n",
            "\u001b[91m☒\u001b[0m 2024-October-17 Thu = 01/11/201/202/11/201/202/11/201/(10/17/2024, thursday)\n",
            "2026-April-23 Thu\n",
            "\u001b[91m☒\u001b[0m 2026-April-23 Thu = 04/24/2024, thursday(04/23/2026, thursday)\n",
            "2025-November-01 Sat\n",
            "\u001b[91m☒\u001b[0m 2025-November-01 Sat = 01/11/201/11/201/11/201/11/201/1(11/01/2025, saturday)\n",
            "Epoch 11 Loss 0.503909, Elapsed_time =58.14 sec\n",
            "2005-March-19 Sat\n",
            "\u001b[91m☒\u001b[0m 2005-March-19 Sat = 03/06/2005, saturday(03/19/2005, saturday)\n",
            "2027-July-14 Wed\n",
            "\u001b[91m☒\u001b[0m 2027-July-14 Wed = 04/24/2027, saturday(07/14/2027, wednesday)\n",
            "2018-November-09 Fri\n",
            "\u001b[91m☒\u001b[0m 2018-November-09 Fri = 11/11/2016, tuesday(11/09/2018, friday)\n",
            "2020-December-18 Fri\n",
            "\u001b[91m☒\u001b[0m 2020-December-18 Fri = 12/12/2022, tuesday(12/18/2020, friday)\n",
            "2028-September-08 Fri\n",
            "\u001b[91m☒\u001b[0m 2028-September-08 Fri = 09/09/2024, saturday(09/08/2028, friday)\n",
            "2008-February-12 Tue\n",
            "\u001b[91m☒\u001b[0m 2008-February-12 Tue = 02/20/2002, thursday(02/12/2008, tuesday)\n",
            "2019-June-13 Thu\n",
            "\u001b[91m☒\u001b[0m 2019-June-13 Thu = 01/16/2017, tuesday(06/13/2019, thursday)\n",
            "2013-March-31 Sun\n",
            "\u001b[91m☒\u001b[0m 2013-March-31 Sun = 01/11/2012, tuesday(03/31/2013, sunday)\n",
            "2020-November-20 Fri\n",
            "\u001b[91m☒\u001b[0m 2020-November-20 Fri = 11/11/2012, tuesday(11/20/2020, friday)\n",
            "2005-September-03 Sat\n",
            "\u001b[91m☒\u001b[0m 2005-September-03 Sat = 09/04/2007, saturday(09/03/2005, saturday)\n",
            "Epoch 12 Loss 0.368251, Elapsed_time =57.80 sec\n",
            "2016-September-17 Sat\n",
            "\u001b[91m☒\u001b[0m 2016-September-17 Sat = 09/14/2019, tuesday(09/17/2016, saturday)\n",
            "2027-January-20 Wed\n",
            "\u001b[91m☒\u001b[0m 2027-January-20 Wed = 01/11/2026, sunday(01/20/2027, wednesday)\n",
            "2008-May-11 Sun\n",
            "\u001b[91m☒\u001b[0m 2008-May-11 Sun = 01/06/2007, saturday(05/11/2008, sunday)\n",
            "2003-August-05 Tue\n",
            "\u001b[91m☒\u001b[0m 2003-August-05 Tue = 08/04/2007, tuesday(08/05/2003, tuesday)\n",
            "2033-February-13 Sun\n",
            "\u001b[91m☒\u001b[0m 2033-February-13 Sun = 02/12/2022022, tuesday(02/13/2033, sunday)\n",
            "2012-November-23 Fri\n",
            "\u001b[91m☒\u001b[0m 2012-November-23 Fri = 11/11/2012, tuesday(11/23/2012, friday)\n",
            "2012-August-16 Thu\n",
            "\u001b[91m☒\u001b[0m 2012-August-16 Thu = 08/06/2026, tuesday(08/16/2012, thursday)\n",
            "2003-October-01 Wed\n",
            "\u001b[91m☒\u001b[0m 2003-October-01 Wed = 01/11/2012, sunday(10/01/2003, wednesday)\n",
            "2032-November-26 Fri\n",
            "\u001b[91m☒\u001b[0m 2032-November-26 Fri = 11/11/2012, tuesday(11/26/2032, friday)\n",
            "2028-November-11 Sat\n",
            "\u001b[91m☒\u001b[0m 2028-November-11 Sat = 11/11/2012, thursday(11/11/2028, saturday)\n",
            "Epoch 13 Loss 0.347914, Elapsed_time =57.61 sec\n",
            "2019-April-13 Sat\n",
            "\u001b[91m☒\u001b[0m 2019-April-13 Sat = 04/20/2014, tuesday(04/13/2019, saturday)\n",
            "2010-April-12 Mon\n",
            "\u001b[91m☒\u001b[0m 2010-April-12 Mon = 04/20/2014, tuesday(04/12/2010, monday)\n",
            "2028-May-17 Wed\n",
            "\u001b[91m☒\u001b[0m 2028-May-17 Wed = 06/24/2027, saturday(05/17/2028, wednesday)\n",
            "2008-February-15 Fri\n",
            "\u001b[91m☒\u001b[0m 2008-February-15 Fri = 02/02/2009, thursday(02/15/2008, friday)\n",
            "2022-April-11 Mon\n",
            "\u001b[91m☒\u001b[0m 2022-April-11 Mon = 01/14/202/14/202/14/202/14/202/1(04/11/2022, monday)\n",
            "2004-April-20 Tue\n",
            "\u001b[91m☒\u001b[0m 2004-April-20 Tue = 04/20/2004, saturday(04/20/2004, tuesday)\n",
            "2013-September-26 Thu\n",
            "\u001b[91m☒\u001b[0m 2013-September-26 Thu = 09/20/2012, tuesday(09/26/2013, thursday)\n",
            "2033-January-17 Mon\n",
            "\u001b[91m☒\u001b[0m 2033-January-17 Mon = 01/11/2012, tuesday(01/17/2033, monday)\n",
            "2029-June-14 Thu\n",
            "\u001b[91m☒\u001b[0m 2029-June-14 Thu = 06/24/2026, saturday(06/14/2029, thursday)\n",
            "2032-July-16 Fri\n",
            "\u001b[91m☒\u001b[0m 2032-July-16 Fri = 06/24/2026, saturday(07/16/2032, friday)\n",
            "Epoch 14 Loss 0.326531, Elapsed_time =57.17 sec\n",
            "2004-March-09 Tue\n",
            "\u001b[91m☒\u001b[0m 2004-March-09 Tue = 03/04/2004, thursday(03/09/2004, tuesday)\n",
            "2017-November-21 Tue\n",
            "\u001b[91m☒\u001b[0m 2017-November-21 Tue = 11/11/2016, tuesday(11/21/2017, tuesday)\n",
            "2006-December-20 Wed\n",
            "\u001b[91m☒\u001b[0m 2006-December-20 Wed = 12/20/2012, thursday(12/20/2006, wednesday)\n",
            "2002-March-17 Sun\n",
            "\u001b[91m☒\u001b[0m 2002-March-17 Sun = 03/03/2023, saturday(03/17/2002, sunday)\n",
            "2003-January-28 Tue\n",
            "\u001b[91m☒\u001b[0m 2003-January-28 Tue = 01/06/2003, saturday(01/28/2003, tuesday)\n",
            "2010-February-14 Sun\n",
            "\u001b[91m☒\u001b[0m 2010-February-14 Sun = 02/12/2012, tuesday(02/14/2010, sunday)\n",
            "2032-June-30 Wed\n",
            "\u001b[91m☒\u001b[0m 2032-June-30 Wed = 06/06/2023, saturday(06/30/2032, wednesday)\n",
            "2016-May-12 Thu\n",
            "\u001b[91m☒\u001b[0m 2016-May-12 Thu = 01/25/2016, thursday(05/12/2016, thursday)\n",
            "2028-December-27 Wed\n",
            "\u001b[91m☒\u001b[0m 2028-December-27 Wed = 02/12/2029, thursday(12/27/2028, wednesday)\n",
            "2015-September-30 Wed\n",
            "\u001b[91m☒\u001b[0m 2015-September-30 Wed = 09/24/2017, thursday(09/30/2015, wednesday)\n",
            "Epoch 15 Loss 0.309796, Elapsed_time =58.29 sec\n",
            "2026-November-11 Wed\n",
            "\u001b[91m☒\u001b[0m 2026-November-11 Wed = 11/11/2012, thursday(11/11/2026, wednesday)\n",
            "2031-January-07 Tue\n",
            "\u001b[91m☒\u001b[0m 2031-January-07 Tue = 01/01/2011, tuesday(01/07/2031, tuesday)\n",
            "2016-September-27 Tue\n",
            "\u001b[91m☒\u001b[0m 2016-September-27 Tue = 09/24/2014, tuesday(09/27/2016, tuesday)\n",
            "2030-February-15 Fri\n",
            "\u001b[91m☒\u001b[0m 2030-February-15 Fri = 02/02/2002, thursday(02/15/2030, friday)\n",
            "2013-February-17 Sun\n",
            "\u001b[91m☒\u001b[0m 2013-February-17 Sun = 02/12/2012, tuesday(02/17/2013, sunday)\n",
            "2008-January-02 Wed\n",
            "\u001b[91m☒\u001b[0m 2008-January-02 Wed = 01/06/2007, saturday(01/02/2008, wednesday)\n",
            "2014-May-17 Sat\n",
            "\u001b[91m☒\u001b[0m 2014-May-17 Sat = 05/06/2014, thursday(05/17/2014, saturday)\n",
            "2022-November-26 Sat\n",
            "\u001b[91m☒\u001b[0m 2022-November-26 Sat = 11/11/2022, thursday(11/26/2022, saturday)\n",
            "2016-September-29 Thu\n",
            "\u001b[91m☒\u001b[0m 2016-September-29 Thu = 09/24/2014, tuesday(09/29/2016, thursday)\n",
            "2030-April-05 Fri\n",
            "\u001b[91m☒\u001b[0m 2030-April-05 Fri = 04/04/2004, saturday(04/05/2030, friday)\n",
            "Epoch 16 Loss 0.295437, Elapsed_time =57.62 sec\n",
            "2012-July-06 Fri\n",
            "\u001b[91m☒\u001b[0m 2012-July-06 Fri = 07/24/2012, thursday(07/06/2012, friday)\n",
            "2007-June-17 Sun\n",
            "\u001b[91m☒\u001b[0m 2007-June-17 Sun = 06/07/2007, saturday(06/17/2007, sunday)\n",
            "2028-October-28 Sat\n",
            "\u001b[91m☒\u001b[0m 2028-October-28 Sat = 10/10/2028, thursday(10/28/2028, saturday)\n",
            "2024-December-04 Wed\n",
            "\u001b[91m☒\u001b[0m 2024-December-04 Wed = 02/09/2009, thursday(12/04/2024, wednesday)\n",
            "2013-May-09 Thu\n",
            "\u001b[91m☒\u001b[0m 2013-May-09 Thu = 05/05/2003, tuesday(05/09/2013, thursday)\n",
            "2023-December-04 Mon\n",
            "\u001b[91m☒\u001b[0m 2023-December-04 Mon = 02/02/2023, saturday(12/04/2023, monday)\n",
            "2014-September-14 Sun\n",
            "\u001b[91m☒\u001b[0m 2014-September-14 Sun = 09/24/2014, thursday(09/14/2014, sunday)\n",
            "2015-March-02 Mon\n",
            "\u001b[91m☒\u001b[0m 2015-March-02 Mon = 03/25/2015, thursday(03/02/2015, monday)\n",
            "2019-November-25 Mon\n",
            "\u001b[91m☒\u001b[0m 2019-November-25 Mon = 11/11/2016, tuesday(11/25/2019, monday)\n",
            "2006-October-08 Sun\n",
            "\u001b[91m☒\u001b[0m 2006-October-08 Sun = 01/10/2005, thursday(10/08/2006, sunday)\n",
            "Epoch 17 Loss 0.278042, Elapsed_time =56.84 sec\n",
            "2014-October-19 Sun\n",
            "\u001b[91m☒\u001b[0m 2014-October-19 Sun = 10/14/2014, saturday(10/19/2014, sunday)\n",
            "2020-May-18 Mon\n",
            "\u001b[91m☒\u001b[0m 2020-May-18 Mon = 05/17/2020, saturday(05/18/2020, monday)\n",
            "2027-September-02 Thu\n",
            "\u001b[91m☒\u001b[0m 2027-September-02 Thu = 09/09/2027, saturday(09/02/2027, thursday)\n",
            "2033-April-29 Fri\n",
            "\u001b[91m☒\u001b[0m 2033-April-29 Fri = 04/24/2033, sunday(04/29/2033, friday)\n",
            "2033-April-23 Sat\n",
            "\u001b[91m☒\u001b[0m 2033-April-23 Sat = 04/24/2033, sunday(04/23/2033, saturday)\n",
            "2026-July-29 Wed\n",
            "\u001b[91m☒\u001b[0m 2026-July-29 Wed = 09/24/2026, friday(07/29/2026, wednesday)\n",
            "2023-December-28 Thu\n",
            "\u001b[91m☒\u001b[0m 2023-December-28 Thu = 02/12/2023, saturday(12/28/2023, thursday)\n",
            "2022-December-24 Sat\n",
            "\u001b[91m☒\u001b[0m 2022-December-24 Sat = 02/12/2022, saturday(12/24/2022, saturday)\n",
            "2029-January-31 Wed\n",
            "\u001b[91m☒\u001b[0m 2029-January-31 Wed = 01/11/2029, saturday(01/31/2029, wednesday)\n",
            "2010-June-14 Mon\n",
            "\u001b[91m☒\u001b[0m 2010-June-14 Mon = 06/16/2016, sunday(06/14/2010, monday)\n",
            "Epoch 18 Loss 0.262458, Elapsed_time =57.17 sec\n",
            "2010-November-18 Thu\n",
            "\u001b[91m☒\u001b[0m 2010-November-18 Thu = 11/11/2010, sunday(11/18/2010, thursday)\n",
            "2016-September-04 Sun\n",
            "\u001b[91m☒\u001b[0m 2016-September-04 Sun = 09/09/2016, saturday(09/04/2016, sunday)\n",
            "2007-January-13 Sat\n",
            "\u001b[91m☒\u001b[0m 2007-January-13 Sat = 01/01/2007, saturday(01/13/2007, saturday)\n",
            "2028-August-01 Tue\n",
            "\u001b[91m☒\u001b[0m 2028-August-01 Tue = 08/18/2028, thursday(08/01/2028, tuesday)\n",
            "2008-January-08 Tue\n",
            "\u001b[91m☒\u001b[0m 2008-January-08 Tue = 01/06/2008, thursday(01/08/2008, tuesday)\n",
            "2031-March-06 Thu\n",
            "\u001b[91m☒\u001b[0m 2031-March-06 Thu = 03/03/2013, thursday(03/06/2031, thursday)\n",
            "2003-September-07 Sun\n",
            "\u001b[91m☒\u001b[0m 2003-September-07 Sun = 09/09/2003, sunday(09/07/2003, sunday)\n",
            "2015-July-08 Wed\n",
            "\u001b[91m☒\u001b[0m 2015-July-08 Wed = 08/25/2015, sunday(07/08/2015, wednesday)\n",
            "2007-April-07 Sat\n",
            "\u001b[91m☒\u001b[0m 2007-April-07 Sat = 04/04/2007, saturday(04/07/2007, saturday)\n",
            "2013-July-07 Sun\n",
            "\u001b[91m☒\u001b[0m 2013-July-07 Sun = 07/20/2013, sunday(07/07/2013, sunday)\n",
            "Epoch 19 Loss 0.243239, Elapsed_time =57.87 sec\n",
            "2006-December-15 Fri\n",
            "\u001b[91m☒\u001b[0m 2006-December-15 Fri = 12/12/2017, thursday(12/15/2006, friday)\n",
            "2019-December-21 Sat\n",
            "\u001b[91m☒\u001b[0m 2019-December-21 Sat = 12/12/2019, tuesday(12/21/2019, saturday)\n",
            "2028-July-13 Thu\n",
            "\u001b[91m☒\u001b[0m 2028-July-13 Thu = 05/10/2028, thursday(07/13/2028, thursday)\n",
            "2010-July-13 Tue\n",
            "\u001b[91m☒\u001b[0m 2010-July-13 Tue = 05/10/2010, tuesday(07/13/2010, tuesday)\n",
            "2027-April-28 Wed\n",
            "\u001b[91m☒\u001b[0m 2027-April-28 Wed = 04/20/2027, saturday(04/28/2027, wednesday)\n",
            "2011-July-07 Thu\n",
            "\u001b[91m☒\u001b[0m 2011-July-07 Thu = 07/05/2016, tuesday(07/07/2011, thursday)\n",
            "2008-May-05 Mon\n",
            "\u001b[91m☒\u001b[0m 2008-May-05 Mon = 05/05/2008, saturday(05/05/2008, monday)\n",
            "2008-January-23 Wed\n",
            "\u001b[91m☒\u001b[0m 2008-January-23 Wed = 01/03/2008, saturday(01/23/2008, wednesday)\n",
            "2028-December-24 Sun\n",
            "\u001b[91m☒\u001b[0m 2028-December-24 Sun = 12/29/2028, thursday(12/24/2028, sunday)\n",
            "2022-October-25 Tue\n",
            "\u001b[91m☒\u001b[0m 2022-October-25 Tue = 10/20/2022, tuesday(10/25/2022, tuesday)\n",
            "Epoch 20 Loss 0.220727, Elapsed_time =57.32 sec\n",
            "2002-December-14 Sat\n",
            "\u001b[91m☒\u001b[0m 2002-December-14 Sat = 12/12/2020, saturday(12/14/2002, saturday)\n",
            "2024-November-29 Fri\n",
            "\u001b[91m☒\u001b[0m 2024-November-29 Fri = 11/24/2024, saturday(11/29/2024, friday)\n",
            "2026-December-08 Tue\n",
            "\u001b[91m☒\u001b[0m 2026-December-08 Tue = 02/02/2026, thursday(12/08/2026, tuesday)\n",
            "2021-September-22 Wed\n",
            "\u001b[91m☒\u001b[0m 2021-September-22 Wed = 09/20/2021, saturday(09/22/2021, wednesday)\n",
            "2013-June-20 Thu\n",
            "\u001b[91m☒\u001b[0m 2013-June-20 Thu = 05/20/2013, tuesday(06/20/2013, thursday)\n",
            "2029-August-12 Sun\n",
            "\u001b[91m☒\u001b[0m 2029-August-12 Sun = 08/28/2029, saturday(08/12/2029, sunday)\n",
            "2026-February-07 Sat\n",
            "\u001b[91m☒\u001b[0m 2026-February-07 Sat = 02/09/2026, saturday(02/07/2026, saturday)\n",
            "2012-November-20 Tue\n",
            "\u001b[91m☒\u001b[0m 2012-November-20 Tue = 11/21/2012, thursday(11/20/2012, tuesday)\n",
            "2027-November-15 Mon\n",
            "\u001b[91m☒\u001b[0m 2027-November-15 Mon = 11/11/2027, saturday(11/15/2027, monday)\n",
            "2012-July-29 Sun\n",
            "\u001b[91m☒\u001b[0m 2012-July-29 Sun = 07/20/2012, saturday(07/29/2012, sunday)\n",
            "Epoch 21 Loss 0.196625, Elapsed_time =55.39 sec\n",
            "2020-March-25 Wed\n",
            "\u001b[91m☒\u001b[0m 2020-March-25 Wed = 03/25/2020, saturday(03/25/2020, wednesday)\n",
            "2003-September-01 Mon\n",
            "\u001b[91m☒\u001b[0m 2003-September-01 Mon = 09/09/2003, saturday(09/01/2003, monday)\n",
            "2011-July-29 Fri\n",
            "\u001b[91m☒\u001b[0m 2011-July-29 Fri = 09/20/2011, saturday(07/29/2011, friday)\n",
            "2027-November-21 Sun\n",
            "\u001b[91m☒\u001b[0m 2027-November-21 Sun = 11/11/2027, saturday(11/21/2027, sunday)\n",
            "2015-July-17 Fri\n",
            "\u001b[91m☒\u001b[0m 2015-July-17 Fri = 07/17/2015, sunday(07/17/2015, friday)\n",
            "2025-August-14 Thu\n",
            "\u001b[91m☒\u001b[0m 2025-August-14 Thu = 08/18/2025, thursday(08/14/2025, thursday)\n",
            "2026-March-15 Sun\n",
            "\u001b[91m☒\u001b[0m 2026-March-15 Sun = 03/15/2026, saturday(03/15/2026, sunday)\n",
            "2011-June-06 Mon\n",
            "\u001b[91m☒\u001b[0m 2011-June-06 Mon = 06/10/2011, saturday(06/06/2011, monday)\n",
            "2007-December-21 Fri\n",
            "\u001b[91m☒\u001b[0m 2007-December-21 Fri = 12/12/2007, sunday(12/21/2007, friday)\n",
            "2029-April-02 Mon\n",
            "\u001b[91m☒\u001b[0m 2029-April-02 Mon = 04/20/2029, saturday(04/02/2029, monday)\n",
            "Epoch 22 Loss 0.184703, Elapsed_time =55.57 sec\n",
            "2027-February-07 Sun\n",
            "\u001b[91m☒\u001b[0m 2027-February-07 Sun = 02/09/2027, saturday(02/07/2027, sunday)\n",
            "2008-February-01 Fri\n",
            "\u001b[91m☒\u001b[0m 2008-February-01 Fri = 02/02/2008, saturday(02/01/2008, friday)\n",
            "2027-September-22 Wed\n",
            "\u001b[91m☒\u001b[0m 2027-September-22 Wed = 09/202027, saturday(09/22/2027, wednesday)\n",
            "2008-April-13 Sun\n",
            "\u001b[91m☒\u001b[0m 2008-April-13 Sun = 04/10/2008, saturday(04/13/2008, sunday)\n",
            "2008-April-02 Wed\n",
            "\u001b[91m☒\u001b[0m 2008-April-02 Wed = 04/20/2008, saturday(04/02/2008, wednesday)\n",
            "2006-April-21 Fri\n",
            "\u001b[91m☒\u001b[0m 2006-April-21 Fri = 04/20/2006, saturday(04/21/2006, friday)\n",
            "2006-October-31 Tue\n",
            "\u001b[91m☒\u001b[0m 2006-October-31 Tue = 10/10/2010, saturday(10/31/2006, tuesday)\n",
            "2011-February-09 Wed\n",
            "\u001b[91m☒\u001b[0m 2011-February-09 Wed = 02/09/2011, saturday(02/09/2011, wednesday)\n",
            "2030-December-09 Mon\n",
            "\u001b[91m☒\u001b[0m 2030-December-09 Mon = 02/09/2003, sunday(12/09/2030, monday)\n",
            "2019-March-01 Fri\n",
            "\u001b[91m☒\u001b[0m 2019-March-01 Fri = 03/10/2019, saturday(03/01/2019, friday)\n",
            "Epoch 23 Loss 0.174163, Elapsed_time =56.15 sec\n",
            "2007-May-23 Wed\n",
            "\u001b[91m☒\u001b[0m 2007-May-23 Wed = 03/2007, sunday(05/23/2007, wednesday)\n",
            "2022-August-08 Mon\n",
            "\u001b[91m☒\u001b[0m 2022-August-08 Mon = 08/08/2022, saturday(08/08/2022, monday)\n",
            "2008-August-24 Sun\n",
            "\u001b[91m☒\u001b[0m 2008-August-24 Sun = 08/28/2008, saturday(08/24/2008, sunday)\n",
            "2002-January-29 Tue\n",
            "\u001b[91m☒\u001b[0m 2002-January-29 Tue = 01/20/2020, saturday(01/29/2002, tuesday)\n",
            "2003-September-19 Fri\n",
            "\u001b[92m☑\u001b[0m 2003-September-19 Fri = 09/19/2003, friday(09/19/2003, friday)\n",
            "2002-January-10 Thu\n",
            "\u001b[91m☒\u001b[0m 2002-January-10 Thu = 01/01/2002, tuesday(01/10/2002, thursday)\n",
            "2008-December-13 Sat\n",
            "\u001b[92m☑\u001b[0m 2008-December-13 Sat = 12/13/2008, saturday(12/13/2008, saturday)\n",
            "2022-February-24 Thu\n",
            "\u001b[91m☒\u001b[0m 2022-February-24 Thu = 02/2022, tuesday(02/24/2022, thursday)\n",
            "2013-January-11 Fri\n",
            "\u001b[91m☒\u001b[0m 2013-January-11 Fri = 01/11/2013, sunday(01/11/2013, friday)\n",
            "2029-August-19 Sun\n",
            "\u001b[91m☒\u001b[0m 2029-August-19 Sun = 08/19/2029, saturday(08/19/2029, sunday)\n",
            "Epoch 24 Loss 0.136142, Elapsed_time =55.83 sec\n",
            "2023-May-22 Mon\n",
            "\u001b[91m☒\u001b[0m 2023-May-22 Mon = 02/22/2023, sunday(05/22/2023, monday)\n",
            "2021-October-07 Thu\n",
            "\u001b[91m☒\u001b[0m 2021-October-07 Thu = 01/07/2012, tuesday(10/07/2021, thursday)\n",
            "2004-October-31 Sun\n",
            "\u001b[91m☒\u001b[0m 2004-October-31 Sun = 10/13/2004, saturday(10/31/2004, sunday)\n",
            "2026-September-22 Tue\n",
            "\u001b[92m☑\u001b[0m 2026-September-22 Tue = 09/22/2026, tuesday(09/22/2026, tuesday)\n",
            "2010-July-19 Mon\n",
            "\u001b[91m☒\u001b[0m 2010-July-19 Mon = 07/17/2010, sunday(07/19/2010, monday)\n",
            "2028-October-10 Tue\n",
            "\u001b[92m☑\u001b[0m 2028-October-10 Tue = 10/10/2028, tuesday(10/10/2028, tuesday)\n",
            "2014-September-01 Mon\n",
            "\u001b[91m☒\u001b[0m 2014-September-01 Mon = 09/10/2014, saturday(09/01/2014, monday)\n",
            "2028-July-25 Tue\n",
            "\u001b[92m☑\u001b[0m 2028-July-25 Tue = 07/25/2028, tuesday(07/25/2028, tuesday)\n",
            "2026-June-25 Thu\n",
            "\u001b[92m☑\u001b[0m 2026-June-25 Thu = 06/25/2026, thursday(06/25/2026, thursday)\n",
            "2002-March-31 Sun\n",
            "\u001b[91m☒\u001b[0m 2002-March-31 Sun = 03/03/2002, saturday(03/31/2002, sunday)\n",
            "Epoch 25 Loss 0.103908, Elapsed_time =55.51 sec\n",
            "2029-December-13 Thu\n",
            "\u001b[91m☒\u001b[0m 2029-December-13 Thu = 12/13/2029, tuesday(12/13/2029, thursday)\n",
            "2015-March-01 Sun\n",
            "\u001b[91m☒\u001b[0m 2015-March-01 Sun = 03/10/2015, saturday(03/01/2015, sunday)\n",
            "2027-February-26 Fri\n",
            "\u001b[92m☑\u001b[0m 2027-February-26 Fri = 02/26/2027, friday(02/26/2027, friday)\n",
            "2015-March-09 Mon\n",
            "\u001b[91m☒\u001b[0m 2015-March-09 Mon = 03/09/2015, sunday(03/09/2015, monday)\n",
            "2013-March-12 Tue\n",
            "\u001b[91m☒\u001b[0m 2013-March-12 Tue = 03/13/2013, thursday(03/12/2013, tuesday)\n",
            "2012-May-28 Mon\n",
            "\u001b[91m☒\u001b[0m 2012-May-28 Mon = 05/28/2012, sunday(05/28/2012, monday)\n",
            "2032-November-06 Sat\n",
            "\u001b[92m☑\u001b[0m 2032-November-06 Sat = 11/06/2032, saturday(11/06/2032, saturday)\n",
            "2031-August-18 Mon\n",
            "\u001b[92m☑\u001b[0m 2031-August-18 Mon = 08/18/2031, monday(08/18/2031, monday)\n",
            "2012-August-05 Sun\n",
            "\u001b[91m☒\u001b[0m 2012-August-05 Sun = 08/05/2012, saturday(08/05/2012, sunday)\n",
            "2017-November-06 Mon\n",
            "\u001b[91m☒\u001b[0m 2017-November-06 Mon = 11/16/2017, monday(11/06/2017, monday)\n",
            "Epoch 26 Loss 0.073508, Elapsed_time =56.96 sec\n",
            "2008-December-16 Tue\n",
            "\u001b[92m☑\u001b[0m 2008-December-16 Tue = 12/16/2008, tuesday(12/16/2008, tuesday)\n",
            "2019-February-14 Thu\n",
            "\u001b[92m☑\u001b[0m 2019-February-14 Thu = 02/14/2019, thursday(02/14/2019, thursday)\n",
            "2008-July-20 Sun\n",
            "\u001b[92m☑\u001b[0m 2008-July-20 Sun = 07/20/2008, sunday(07/20/2008, sunday)\n",
            "2015-June-25 Thu\n",
            "\u001b[92m☑\u001b[0m 2015-June-25 Thu = 06/25/2015, thursday(06/25/2015, thursday)\n",
            "2033-February-16 Wed\n",
            "\u001b[91m☒\u001b[0m 2033-February-16 Wed = 023, friday(02/16/2033, wednesday)\n",
            "2031-June-13 Fri\n",
            "\u001b[91m☒\u001b[0m 2031-June-13 Fri = 06/13/2031, monday(06/13/2031, friday)\n",
            "2019-December-08 Sun\n",
            "\u001b[92m☑\u001b[0m 2019-December-08 Sun = 12/08/2019, sunday(12/08/2019, sunday)\n",
            "2031-April-16 Wed\n",
            "\u001b[91m☒\u001b[0m 2031-April-16 Wed = 04/16/2031, friday(04/16/2031, wednesday)\n",
            "2020-January-19 Sun\n",
            "\u001b[91m☒\u001b[0m 2020-January-19 Sun = 01/19/2020, saturday(01/19/2020, sunday)\n",
            "2002-July-02 Tue\n",
            "\u001b[91m☒\u001b[0m 2002-July-02 Tue = 07/20/2002, tuesday(07/02/2002, tuesday)\n",
            "Epoch 27 Loss 0.051919, Elapsed_time =56.97 sec\n",
            "2007-December-22 Sat\n",
            "\u001b[91m☒\u001b[0m 2007-December-22 Sat = 12/22007, sunday(12/22/2007, saturday)\n",
            "2007-April-16 Mon\n",
            "\u001b[92m☑\u001b[0m 2007-April-16 Mon = 04/16/2007, monday(04/16/2007, monday)\n",
            "2003-February-08 Sat\n",
            "\u001b[92m☑\u001b[0m 2003-February-08 Sat = 02/08/2003, saturday(02/08/2003, saturday)\n",
            "2019-March-02 Sat\n",
            "\u001b[92m☑\u001b[0m 2019-March-02 Sat = 03/02/2019, saturday(03/02/2019, saturday)\n",
            "2020-January-13 Mon\n",
            "\u001b[91m☒\u001b[0m 2020-January-13 Mon = 01/13/2020, saturday(01/13/2020, monday)\n",
            "2016-January-20 Wed\n",
            "\u001b[91m☒\u001b[0m 2016-January-20 Wed = 01/2016, friday(01/20/2016, wednesday)\n",
            "2030-July-19 Fri\n",
            "\u001b[92m☑\u001b[0m 2030-July-19 Fri = 07/19/2030, friday(07/19/2030, friday)\n",
            "2010-September-29 Wed\n",
            "\u001b[91m☒\u001b[0m 2010-September-29 Wed = 09/29/2010, friday(09/29/2010, wednesday)\n",
            "2029-November-10 Sat\n",
            "\u001b[92m☑\u001b[0m 2029-November-10 Sat = 11/10/2029, saturday(11/10/2029, saturday)\n",
            "2011-August-04 Thu\n",
            "\u001b[92m☑\u001b[0m 2011-August-04 Thu = 08/04/2011, thursday(08/04/2011, thursday)\n",
            "Epoch 28 Loss 0.038907, Elapsed_time =58.03 sec\n",
            "2028-January-17 Mon\n",
            "\u001b[92m☑\u001b[0m 2028-January-17 Mon = 01/17/2028, monday(01/17/2028, monday)\n",
            "2005-June-12 Sun\n",
            "\u001b[92m☑\u001b[0m 2005-June-12 Sun = 06/12/2005, sunday(06/12/2005, sunday)\n",
            "2013-April-12 Fri\n",
            "\u001b[92m☑\u001b[0m 2013-April-12 Fri = 04/12/2013, friday(04/12/2013, friday)\n",
            "2018-July-03 Tue\n",
            "\u001b[92m☑\u001b[0m 2018-July-03 Tue = 07/03/2018, tuesday(07/03/2018, tuesday)\n",
            "2024-February-07 Wed\n",
            "\u001b[91m☒\u001b[0m 2024-February-07 Wed = 02/07/2024, friday(02/07/2024, wednesday)\n",
            "2009-May-30 Sat\n",
            "\u001b[91m☒\u001b[0m 2009-May-30 Sat = 05/30/2009, sunday(05/30/2009, saturday)\n",
            "2033-May-06 Fri\n",
            "\u001b[92m☑\u001b[0m 2033-May-06 Fri = 05/06/2033, friday(05/06/2033, friday)\n",
            "2005-September-07 Wed\n",
            "\u001b[91m☒\u001b[0m 2005-September-07 Wed = 09/07/2005, friday(09/07/2005, wednesday)\n",
            "2017-October-18 Wed\n",
            "\u001b[91m☒\u001b[0m 2017-October-18 Wed = 10/18/2017, friday(10/18/2017, wednesday)\n",
            "2003-April-04 Fri\n",
            "\u001b[92m☑\u001b[0m 2003-April-04 Fri = 04/04/2003, friday(04/04/2003, friday)\n",
            "Epoch 29 Loss 0.028701, Elapsed_time =56.48 sec\n",
            "2026-June-30 Tue\n",
            "\u001b[91m☒\u001b[0m 2026-June-30 Tue = 06/2026, tuesday(06/30/2026, tuesday)\n",
            "2006-November-20 Mon\n",
            "\u001b[92m☑\u001b[0m 2006-November-20 Mon = 11/20/2006, monday(11/20/2006, monday)\n",
            "2011-February-24 Thu\n",
            "\u001b[91m☒\u001b[0m 2011-February-24 Thu = 02/24/2012, thursday(02/24/2011, thursday)\n",
            "2022-January-26 Wed\n",
            "\u001b[91m☒\u001b[0m 2022-January-26 Wed = 01/26/2022, friday(01/26/2022, wednesday)\n",
            "2026-July-08 Wed\n",
            "\u001b[91m☒\u001b[0m 2026-July-08 Wed = 07/08/2026, friday(07/08/2026, wednesday)\n",
            "2012-July-25 Wed\n",
            "\u001b[91m☒\u001b[0m 2012-July-25 Wed = 07/25/2012, friday(07/25/2012, wednesday)\n",
            "2014-December-01 Mon\n",
            "\u001b[92m☑\u001b[0m 2014-December-01 Mon = 12/01/2014, monday(12/01/2014, monday)\n",
            "2007-November-28 Wed\n",
            "\u001b[91m☒\u001b[0m 2007-November-28 Wed = 11/28/2007, friday(11/28/2007, wednesday)\n",
            "2005-October-14 Fri\n",
            "\u001b[92m☑\u001b[0m 2005-October-14 Fri = 10/14/2005, friday(10/14/2005, friday)\n",
            "2009-June-15 Mon\n",
            "\u001b[92m☑\u001b[0m 2009-June-15 Mon = 06/15/2009, monday(06/15/2009, monday)\n",
            "Epoch 30 Loss 0.021402, Elapsed_time =56.89 sec\n",
            "2015-October-12 Mon\n",
            "\u001b[92m☑\u001b[0m 2015-October-12 Mon = 10/12/2015, monday(10/12/2015, monday)\n",
            "2016-December-16 Fri\n",
            "\u001b[92m☑\u001b[0m 2016-December-16 Fri = 12/16/2016, friday(12/16/2016, friday)\n",
            "2004-June-22 Tue\n",
            "\u001b[92m☑\u001b[0m 2004-June-22 Tue = 06/22/2004, tuesday(06/22/2004, tuesday)\n",
            "2024-November-18 Mon\n",
            "\u001b[92m☑\u001b[0m 2024-November-18 Mon = 11/18/2024, monday(11/18/2024, monday)\n",
            "2022-February-09 Wed\n",
            "\u001b[91m☒\u001b[0m 2022-February-09 Wed = 02/09/2022, friday(02/09/2022, wednesday)\n",
            "2016-April-10 Sun\n",
            "\u001b[91m☒\u001b[0m 2016-April-10 Sun = 04/10/2016, saturday(04/10/2016, sunday)\n",
            "2018-May-14 Mon\n",
            "\u001b[92m☑\u001b[0m 2018-May-14 Mon = 05/14/2018, monday(05/14/2018, monday)\n",
            "2010-February-08 Mon\n",
            "\u001b[92m☑\u001b[0m 2010-February-08 Mon = 02/08/2010, monday(02/08/2010, monday)\n",
            "2016-September-15 Thu\n",
            "\u001b[92m☑\u001b[0m 2016-September-15 Thu = 09/15/2016, thursday(09/15/2016, thursday)\n",
            "2029-November-30 Fri\n",
            "\u001b[91m☒\u001b[0m 2029-November-30 Fri = 11/30/2029, wednesday(11/30/2029, friday)\n",
            "Epoch 31 Loss 0.074865, Elapsed_time =56.82 sec\n",
            "2028-September-18 Mon\n",
            "\u001b[92m☑\u001b[0m 2028-September-18 Mon = 09/18/2028, monday(09/18/2028, monday)\n",
            "2019-February-15 Fri\n",
            "\u001b[91m☒\u001b[0m 2019-February-15 Fri = 02/15/2019, wednesday(02/15/2019, friday)\n",
            "2031-April-04 Fri\n",
            "\u001b[92m☑\u001b[0m 2031-April-04 Fri = 04/04/2031, friday(04/04/2031, friday)\n",
            "2025-February-10 Mon\n",
            "\u001b[92m☑\u001b[0m 2025-February-10 Mon = 02/10/2025, monday(02/10/2025, monday)\n",
            "2019-July-09 Tue\n",
            "\u001b[92m☑\u001b[0m 2019-July-09 Tue = 07/09/2019, tuesday(07/09/2019, tuesday)\n",
            "2025-November-18 Tue\n",
            "\u001b[92m☑\u001b[0m 2025-November-18 Tue = 11/18/2025, tuesday(11/18/2025, tuesday)\n",
            "2031-August-04 Mon\n",
            "\u001b[92m☑\u001b[0m 2031-August-04 Mon = 08/04/2031, monday(08/04/2031, monday)\n",
            "2031-August-05 Tue\n",
            "\u001b[92m☑\u001b[0m 2031-August-05 Tue = 08/05/2031, tuesday(08/05/2031, tuesday)\n",
            "2022-November-17 Thu\n",
            "\u001b[92m☑\u001b[0m 2022-November-17 Thu = 11/17/2022, thursday(11/17/2022, thursday)\n",
            "2001-November-23 Fri\n",
            "\u001b[92m☑\u001b[0m 2001-November-23 Fri = 11/23/2001, friday(11/23/2001, friday)\n",
            "Epoch 32 Loss 0.024452, Elapsed_time =57.58 sec\n",
            "2025-March-15 Sat\n",
            "\u001b[92m☑\u001b[0m 2025-March-15 Sat = 03/15/2025, saturday(03/15/2025, saturday)\n",
            "2003-November-24 Mon\n",
            "\u001b[92m☑\u001b[0m 2003-November-24 Mon = 11/24/2003, monday(11/24/2003, monday)\n",
            "2017-March-19 Sun\n",
            "\u001b[92m☑\u001b[0m 2017-March-19 Sun = 03/19/2017, sunday(03/19/2017, sunday)\n",
            "2011-March-10 Thu\n",
            "\u001b[92m☑\u001b[0m 2011-March-10 Thu = 03/10/2011, thursday(03/10/2011, thursday)\n",
            "2006-July-05 Wed\n",
            "\u001b[91m☒\u001b[0m 2006-July-05 Wed = 07/05/2006, friday(07/05/2006, wednesday)\n",
            "2031-January-09 Thu\n",
            "\u001b[92m☑\u001b[0m 2031-January-09 Thu = 01/09/2031, thursday(01/09/2031, thursday)\n",
            "2020-January-15 Wed\n",
            "\u001b[91m☒\u001b[0m 2020-January-15 Wed = 01/15/2020, monday(01/15/2020, wednesday)\n",
            "2008-February-18 Mon\n",
            "\u001b[92m☑\u001b[0m 2008-February-18 Mon = 02/18/2008, monday(02/18/2008, monday)\n",
            "2008-February-27 Wed\n",
            "\u001b[91m☒\u001b[0m 2008-February-27 Wed = 02/27/2008, friday(02/27/2008, wednesday)\n",
            "2024-September-11 Wed\n",
            "\u001b[91m☒\u001b[0m 2024-September-11 Wed = 09/11/2024, friday(09/11/2024, wednesday)\n",
            "Epoch 33 Loss 0.016045, Elapsed_time =58.16 sec\n",
            "2032-November-02 Tue\n",
            "\u001b[92m☑\u001b[0m 2032-November-02 Tue = 11/02/2032, tuesday(11/02/2032, tuesday)\n",
            "2025-December-21 Sun\n",
            "\u001b[92m☑\u001b[0m 2025-December-21 Sun = 12/21/2025, sunday(12/21/2025, sunday)\n",
            "2026-November-13 Fri\n",
            "\u001b[92m☑\u001b[0m 2026-November-13 Fri = 11/13/2026, friday(11/13/2026, friday)\n",
            "2015-November-07 Sat\n",
            "\u001b[92m☑\u001b[0m 2015-November-07 Sat = 11/07/2015, saturday(11/07/2015, saturday)\n",
            "2010-July-27 Tue\n",
            "\u001b[92m☑\u001b[0m 2010-July-27 Tue = 07/27/2010, tuesday(07/27/2010, tuesday)\n",
            "2002-August-30 Fri\n",
            "\u001b[92m☑\u001b[0m 2002-August-30 Fri = 08/30/2002, friday(08/30/2002, friday)\n",
            "2007-May-27 Sun\n",
            "\u001b[92m☑\u001b[0m 2007-May-27 Sun = 05/27/2007, sunday(05/27/2007, sunday)\n",
            "2030-April-23 Tue\n",
            "\u001b[92m☑\u001b[0m 2030-April-23 Tue = 04/23/2030, tuesday(04/23/2030, tuesday)\n",
            "2006-September-25 Mon\n",
            "\u001b[92m☑\u001b[0m 2006-September-25 Mon = 09/25/2006, monday(09/25/2006, monday)\n",
            "2015-March-18 Wed\n",
            "\u001b[91m☒\u001b[0m 2015-March-18 Wed = 03/18/2015, friday(03/18/2015, wednesday)\n",
            "Epoch 34 Loss 0.012865, Elapsed_time =57.98 sec\n",
            "2014-May-30 Fri\n",
            "\u001b[92m☑\u001b[0m 2014-May-30 Fri = 05/30/2014, friday(05/30/2014, friday)\n",
            "2005-April-23 Sat\n",
            "\u001b[92m☑\u001b[0m 2005-April-23 Sat = 04/23/2005, saturday(04/23/2005, saturday)\n",
            "2022-February-22 Tue\n",
            "\u001b[92m☑\u001b[0m 2022-February-22 Tue = 02/22/2022, tuesday(02/22/2022, tuesday)\n",
            "2022-May-07 Sat\n",
            "\u001b[92m☑\u001b[0m 2022-May-07 Sat = 05/07/2022, saturday(05/07/2022, saturday)\n",
            "2027-April-29 Thu\n",
            "\u001b[92m☑\u001b[0m 2027-April-29 Thu = 04/29/2027, thursday(04/29/2027, thursday)\n",
            "2006-February-11 Sat\n",
            "\u001b[92m☑\u001b[0m 2006-February-11 Sat = 02/11/2006, saturday(02/11/2006, saturday)\n",
            "2020-January-09 Thu\n",
            "\u001b[92m☑\u001b[0m 2020-January-09 Thu = 01/09/2020, thursday(01/09/2020, thursday)\n",
            "2029-July-18 Wed\n",
            "\u001b[92m☑\u001b[0m 2029-July-18 Wed = 07/18/2029, wednesday(07/18/2029, wednesday)\n",
            "2025-September-02 Tue\n",
            "\u001b[92m☑\u001b[0m 2025-September-02 Tue = 09/02/2025, tuesday(09/02/2025, tuesday)\n",
            "2025-May-24 Sat\n",
            "\u001b[92m☑\u001b[0m 2025-May-24 Sat = 05/24/2025, saturday(05/24/2025, saturday)\n",
            "Epoch 35 Loss 0.010596, Elapsed_time =58.67 sec\n",
            "2009-June-12 Fri\n",
            "\u001b[92m☑\u001b[0m 2009-June-12 Fri = 06/12/2009, friday(06/12/2009, friday)\n",
            "2027-June-22 Tue\n",
            "\u001b[92m☑\u001b[0m 2027-June-22 Tue = 06/22/2027, tuesday(06/22/2027, tuesday)\n",
            "2028-May-03 Wed\n",
            "\u001b[92m☑\u001b[0m 2028-May-03 Wed = 05/03/2028, wednesday(05/03/2028, wednesday)\n",
            "2018-November-21 Wed\n",
            "\u001b[92m☑\u001b[0m 2018-November-21 Wed = 11/21/2018, wednesday(11/21/2018, wednesday)\n",
            "2010-January-03 Sun\n",
            "\u001b[91m☒\u001b[0m 2010-January-03 Sun = 01/03/2010, saturday(01/03/2010, sunday)\n",
            "2007-January-01 Mon\n",
            "\u001b[92m☑\u001b[0m 2007-January-01 Mon = 01/01/2007, monday(01/01/2007, monday)\n",
            "2014-March-22 Sat\n",
            "\u001b[92m☑\u001b[0m 2014-March-22 Sat = 03/22/2014, saturday(03/22/2014, saturday)\n",
            "2004-August-28 Sat\n",
            "\u001b[92m☑\u001b[0m 2004-August-28 Sat = 08/28/2004, saturday(08/28/2004, saturday)\n",
            "2003-July-30 Wed\n",
            "\u001b[92m☑\u001b[0m 2003-July-30 Wed = 07/30/2003, wednesday(07/30/2003, wednesday)\n",
            "2023-July-09 Sun\n",
            "\u001b[92m☑\u001b[0m 2023-July-09 Sun = 07/09/2023, sunday(07/09/2023, sunday)\n",
            "Epoch 36 Loss 0.008363, Elapsed_time =56.13 sec\n",
            "2009-December-14 Mon\n",
            "\u001b[92m☑\u001b[0m 2009-December-14 Mon = 12/14/2009, monday(12/14/2009, monday)\n",
            "2019-July-06 Sat\n",
            "\u001b[92m☑\u001b[0m 2019-July-06 Sat = 07/06/2019, saturday(07/06/2019, saturday)\n",
            "2024-April-08 Mon\n",
            "\u001b[92m☑\u001b[0m 2024-April-08 Mon = 04/08/2024, monday(04/08/2024, monday)\n",
            "2029-August-19 Sun\n",
            "\u001b[92m☑\u001b[0m 2029-August-19 Sun = 08/19/2029, sunday(08/19/2029, sunday)\n",
            "2027-June-15 Tue\n",
            "\u001b[92m☑\u001b[0m 2027-June-15 Tue = 06/15/2027, tuesday(06/15/2027, tuesday)\n",
            "2032-April-26 Mon\n",
            "\u001b[92m☑\u001b[0m 2032-April-26 Mon = 04/26/2032, monday(04/26/2032, monday)\n",
            "2023-July-10 Mon\n",
            "\u001b[92m☑\u001b[0m 2023-July-10 Mon = 07/10/2023, monday(07/10/2023, monday)\n",
            "2007-February-11 Sun\n",
            "\u001b[92m☑\u001b[0m 2007-February-11 Sun = 02/11/2007, sunday(02/11/2007, sunday)\n",
            "2003-May-19 Mon\n",
            "\u001b[92m☑\u001b[0m 2003-May-19 Mon = 05/19/2003, monday(05/19/2003, monday)\n",
            "2026-August-31 Mon\n",
            "\u001b[92m☑\u001b[0m 2026-August-31 Mon = 08/31/2026, monday(08/31/2026, monday)\n",
            "Epoch 37 Loss 0.006628, Elapsed_time =57.89 sec\n",
            "2024-December-02 Mon\n",
            "\u001b[92m☑\u001b[0m 2024-December-02 Mon = 12/02/2024, monday(12/02/2024, monday)\n",
            "2028-December-29 Fri\n",
            "\u001b[92m☑\u001b[0m 2028-December-29 Fri = 12/29/2028, friday(12/29/2028, friday)\n",
            "2011-April-25 Mon\n",
            "\u001b[92m☑\u001b[0m 2011-April-25 Mon = 04/25/2011, monday(04/25/2011, monday)\n",
            "2021-July-25 Sun\n",
            "\u001b[92m☑\u001b[0m 2021-July-25 Sun = 07/25/2021, sunday(07/25/2021, sunday)\n",
            "2017-December-06 Wed\n",
            "\u001b[92m☑\u001b[0m 2017-December-06 Wed = 12/06/2017, wednesday(12/06/2017, wednesday)\n",
            "2002-March-24 Sun\n",
            "\u001b[92m☑\u001b[0m 2002-March-24 Sun = 03/24/2002, sunday(03/24/2002, sunday)\n",
            "2013-August-20 Tue\n",
            "\u001b[92m☑\u001b[0m 2013-August-20 Tue = 08/20/2013, tuesday(08/20/2013, tuesday)\n",
            "2005-April-10 Sun\n",
            "\u001b[92m☑\u001b[0m 2005-April-10 Sun = 04/10/2005, sunday(04/10/2005, sunday)\n",
            "2019-February-21 Thu\n",
            "\u001b[92m☑\u001b[0m 2019-February-21 Thu = 02/21/2019, thursday(02/21/2019, thursday)\n",
            "2015-February-13 Fri\n",
            "\u001b[92m☑\u001b[0m 2015-February-13 Fri = 02/13/2015, friday(02/13/2015, friday)\n",
            "Epoch 38 Loss 0.005417, Elapsed_time =57.02 sec\n",
            "2009-August-26 Wed\n",
            "\u001b[92m☑\u001b[0m 2009-August-26 Wed = 08/26/2009, wednesday(08/26/2009, wednesday)\n",
            "2022-October-09 Sun\n",
            "\u001b[92m☑\u001b[0m 2022-October-09 Sun = 10/09/2022, sunday(10/09/2022, sunday)\n",
            "2015-December-17 Thu\n",
            "\u001b[92m☑\u001b[0m 2015-December-17 Thu = 12/17/2015, thursday(12/17/2015, thursday)\n",
            "2031-May-17 Sat\n",
            "\u001b[91m☒\u001b[0m 2031-May-17 Sat = 05/17/2011, saturday(05/17/2031, saturday)\n",
            "2022-June-06 Mon\n",
            "\u001b[92m☑\u001b[0m 2022-June-06 Mon = 06/06/2022, monday(06/06/2022, monday)\n",
            "2021-March-26 Fri\n",
            "\u001b[92m☑\u001b[0m 2021-March-26 Fri = 03/26/2021, friday(03/26/2021, friday)\n",
            "2017-April-17 Mon\n",
            "\u001b[92m☑\u001b[0m 2017-April-17 Mon = 04/17/2017, monday(04/17/2017, monday)\n",
            "2006-October-15 Sun\n",
            "\u001b[92m☑\u001b[0m 2006-October-15 Sun = 10/15/2006, sunday(10/15/2006, sunday)\n",
            "2012-April-17 Tue\n",
            "\u001b[92m☑\u001b[0m 2012-April-17 Tue = 04/17/2012, tuesday(04/17/2012, tuesday)\n",
            "2008-September-01 Mon\n",
            "\u001b[92m☑\u001b[0m 2008-September-01 Mon = 09/01/2008, monday(09/01/2008, monday)\n",
            "Epoch 39 Loss 0.004527, Elapsed_time =56.10 sec\n",
            "2008-April-17 Thu\n",
            "\u001b[92m☑\u001b[0m 2008-April-17 Thu = 04/17/2008, thursday(04/17/2008, thursday)\n",
            "2003-December-30 Tue\n",
            "\u001b[91m☒\u001b[0m 2003-December-30 Tue = 12/30/2030, tuesday(12/30/2003, tuesday)\n",
            "2004-August-10 Tue\n",
            "\u001b[92m☑\u001b[0m 2004-August-10 Tue = 08/10/2004, tuesday(08/10/2004, tuesday)\n",
            "2021-November-15 Mon\n",
            "\u001b[91m☒\u001b[0m 2021-November-15 Mon = 11/15/2011, monday(11/15/2021, monday)\n",
            "2032-November-03 Wed\n",
            "\u001b[92m☑\u001b[0m 2032-November-03 Wed = 11/03/2032, wednesday(11/03/2032, wednesday)\n",
            "2033-May-08 Sun\n",
            "\u001b[92m☑\u001b[0m 2033-May-08 Sun = 05/08/2033, sunday(05/08/2033, sunday)\n",
            "2004-April-25 Sun\n",
            "\u001b[92m☑\u001b[0m 2004-April-25 Sun = 04/25/2004, sunday(04/25/2004, sunday)\n",
            "2004-August-09 Mon\n",
            "\u001b[92m☑\u001b[0m 2004-August-09 Mon = 08/09/2004, monday(08/09/2004, monday)\n",
            "2021-March-02 Tue\n",
            "\u001b[91m☒\u001b[0m 2021-March-02 Tue = 03/02/2012, tuesday(03/02/2021, tuesday)\n",
            "2007-August-21 Tue\n",
            "\u001b[92m☑\u001b[0m 2007-August-21 Tue = 08/21/2007, tuesday(08/21/2007, tuesday)\n",
            "Epoch 40 Loss 0.003829, Elapsed_time =56.60 sec\n",
            "2026-June-20 Sat\n",
            "\u001b[91m☒\u001b[0m 2026-June-20 Sat = 06/2026, saturday(06/20/2026, saturday)\n",
            "2022-May-05 Thu\n",
            "\u001b[92m☑\u001b[0m 2022-May-05 Thu = 05/05/2022, thursday(05/05/2022, thursday)\n",
            "2018-November-03 Sat\n",
            "\u001b[92m☑\u001b[0m 2018-November-03 Sat = 11/03/2018, saturday(11/03/2018, saturday)\n",
            "2014-July-10 Thu\n",
            "\u001b[92m☑\u001b[0m 2014-July-10 Thu = 07/10/2014, thursday(07/10/2014, thursday)\n",
            "2007-June-10 Sun\n",
            "\u001b[92m☑\u001b[0m 2007-June-10 Sun = 06/10/2007, sunday(06/10/2007, sunday)\n",
            "2024-May-05 Sun\n",
            "\u001b[92m☑\u001b[0m 2024-May-05 Sun = 05/05/2024, sunday(05/05/2024, sunday)\n",
            "2014-March-30 Sun\n",
            "\u001b[92m☑\u001b[0m 2014-March-30 Sun = 03/30/2014, sunday(03/30/2014, sunday)\n",
            "2006-July-04 Tue\n",
            "\u001b[92m☑\u001b[0m 2006-July-04 Tue = 07/04/2006, tuesday(07/04/2006, tuesday)\n",
            "2033-May-01 Sun\n",
            "\u001b[92m☑\u001b[0m 2033-May-01 Sun = 05/01/2033, sunday(05/01/2033, sunday)\n",
            "2015-October-28 Wed\n",
            "\u001b[92m☑\u001b[0m 2015-October-28 Wed = 10/28/2015, wednesday(10/28/2015, wednesday)\n",
            "Epoch 41 Loss 0.003277, Elapsed_time =57.07 sec\n",
            "2023-June-17 Sat\n",
            "\u001b[92m☑\u001b[0m 2023-June-17 Sat = 06/17/2023, saturday(06/17/2023, saturday)\n",
            "2005-January-29 Sat\n",
            "\u001b[92m☑\u001b[0m 2005-January-29 Sat = 01/29/2005, saturday(01/29/2005, saturday)\n",
            "2003-June-28 Sat\n",
            "\u001b[92m☑\u001b[0m 2003-June-28 Sat = 06/28/2003, saturday(06/28/2003, saturday)\n",
            "2026-January-15 Thu\n",
            "\u001b[92m☑\u001b[0m 2026-January-15 Thu = 01/15/2026, thursday(01/15/2026, thursday)\n",
            "2006-October-16 Mon\n",
            "\u001b[92m☑\u001b[0m 2006-October-16 Mon = 10/16/2006, monday(10/16/2006, monday)\n",
            "2012-January-19 Thu\n",
            "\u001b[92m☑\u001b[0m 2012-January-19 Thu = 01/19/2012, thursday(01/19/2012, thursday)\n",
            "2006-March-23 Thu\n",
            "\u001b[92m☑\u001b[0m 2006-March-23 Thu = 03/23/2006, thursday(03/23/2006, thursday)\n",
            "2013-January-26 Sat\n",
            "\u001b[92m☑\u001b[0m 2013-January-26 Sat = 01/26/2013, saturday(01/26/2013, saturday)\n",
            "2009-July-23 Thu\n",
            "\u001b[92m☑\u001b[0m 2009-July-23 Thu = 07/23/2009, thursday(07/23/2009, thursday)\n",
            "2011-December-07 Wed\n",
            "\u001b[92m☑\u001b[0m 2011-December-07 Wed = 12/07/2011, wednesday(12/07/2011, wednesday)\n",
            "Epoch 42 Loss 0.002832, Elapsed_time =58.35 sec\n",
            "2008-February-03 Sun\n",
            "\u001b[92m☑\u001b[0m 2008-February-03 Sun = 02/03/2008, sunday(02/03/2008, sunday)\n",
            "2011-February-27 Sun\n",
            "\u001b[92m☑\u001b[0m 2011-February-27 Sun = 02/27/2011, sunday(02/27/2011, sunday)\n",
            "2010-January-03 Sun\n",
            "\u001b[91m☒\u001b[0m 2010-January-03 Sun = 01/03/2010, saturday(01/03/2010, sunday)\n",
            "2013-February-14 Thu\n",
            "\u001b[92m☑\u001b[0m 2013-February-14 Thu = 02/14/2013, thursday(02/14/2013, thursday)\n",
            "2010-October-28 Thu\n",
            "\u001b[92m☑\u001b[0m 2010-October-28 Thu = 10/28/2010, thursday(10/28/2010, thursday)\n",
            "2024-May-14 Tue\n",
            "\u001b[92m☑\u001b[0m 2024-May-14 Tue = 05/14/2024, tuesday(05/14/2024, tuesday)\n",
            "2019-June-01 Sat\n",
            "\u001b[92m☑\u001b[0m 2019-June-01 Sat = 06/01/2019, saturday(06/01/2019, saturday)\n",
            "2022-October-08 Sat\n",
            "\u001b[92m☑\u001b[0m 2022-October-08 Sat = 10/08/2022, saturday(10/08/2022, saturday)\n",
            "2025-November-21 Fri\n",
            "\u001b[92m☑\u001b[0m 2025-November-21 Fri = 11/21/2025, friday(11/21/2025, friday)\n",
            "2021-October-19 Tue\n",
            "\u001b[92m☑\u001b[0m 2021-October-19 Tue = 10/19/2021, tuesday(10/19/2021, tuesday)\n",
            "Epoch 43 Loss 0.002465, Elapsed_time =57.37 sec\n",
            "2029-August-04 Sat\n",
            "\u001b[92m☑\u001b[0m 2029-August-04 Sat = 08/04/2029, saturday(08/04/2029, saturday)\n",
            "2024-July-26 Fri\n",
            "\u001b[92m☑\u001b[0m 2024-July-26 Fri = 07/26/2024, friday(07/26/2024, friday)\n",
            "2031-January-02 Thu\n",
            "\u001b[92m☑\u001b[0m 2031-January-02 Thu = 01/02/2031, thursday(01/02/2031, thursday)\n",
            "2016-July-23 Sat\n",
            "\u001b[92m☑\u001b[0m 2016-July-23 Sat = 07/23/2016, saturday(07/23/2016, saturday)\n",
            "2008-August-06 Wed\n",
            "\u001b[92m☑\u001b[0m 2008-August-06 Wed = 08/06/2008, wednesday(08/06/2008, wednesday)\n",
            "2020-February-04 Tue\n",
            "\u001b[92m☑\u001b[0m 2020-February-04 Tue = 02/04/2020, tuesday(02/04/2020, tuesday)\n",
            "2024-March-02 Sat\n",
            "\u001b[92m☑\u001b[0m 2024-March-02 Sat = 03/02/2024, saturday(03/02/2024, saturday)\n",
            "2031-January-19 Sun\n",
            "\u001b[92m☑\u001b[0m 2031-January-19 Sun = 01/19/2031, sunday(01/19/2031, sunday)\n",
            "2014-October-15 Wed\n",
            "\u001b[92m☑\u001b[0m 2014-October-15 Wed = 10/15/2014, wednesday(10/15/2014, wednesday)\n",
            "2021-December-24 Fri\n",
            "\u001b[92m☑\u001b[0m 2021-December-24 Fri = 12/24/2021, friday(12/24/2021, friday)\n",
            "Epoch 44 Loss 0.002171, Elapsed_time =56.03 sec\n",
            "2002-November-11 Mon\n",
            "\u001b[92m☑\u001b[0m 2002-November-11 Mon = 11/11/2002, monday(11/11/2002, monday)\n",
            "2008-March-03 Mon\n",
            "\u001b[92m☑\u001b[0m 2008-March-03 Mon = 03/03/2008, monday(03/03/2008, monday)\n",
            "2004-February-13 Fri\n",
            "\u001b[92m☑\u001b[0m 2004-February-13 Fri = 02/13/2004, friday(02/13/2004, friday)\n",
            "2023-September-15 Fri\n",
            "\u001b[92m☑\u001b[0m 2023-September-15 Fri = 09/15/2023, friday(09/15/2023, friday)\n",
            "2014-January-23 Thu\n",
            "\u001b[92m☑\u001b[0m 2014-January-23 Thu = 01/23/2014, thursday(01/23/2014, thursday)\n",
            "2006-June-12 Mon\n",
            "\u001b[92m☑\u001b[0m 2006-June-12 Mon = 06/12/2006, monday(06/12/2006, monday)\n",
            "2015-May-23 Sat\n",
            "\u001b[91m☒\u001b[0m 2015-May-23 Sat = 05/23/2015, sunday(05/23/2015, saturday)\n",
            "2013-April-10 Wed\n",
            "\u001b[92m☑\u001b[0m 2013-April-10 Wed = 04/10/2013, wednesday(04/10/2013, wednesday)\n",
            "2017-April-02 Sun\n",
            "\u001b[92m☑\u001b[0m 2017-April-02 Sun = 04/02/2017, sunday(04/02/2017, sunday)\n",
            "2005-August-15 Mon\n",
            "\u001b[92m☑\u001b[0m 2005-August-15 Mon = 08/15/2005, monday(08/15/2005, monday)\n",
            "Epoch 45 Loss 0.001930, Elapsed_time =56.49 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-1cb7d5a294e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mreal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    677\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    678\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[0;32m--> 679\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-ba771e59d3a7>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, hidden, enc_output)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mcontext_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;31m# context_vector: batch_size * enc_unit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# attention_weights: batch_size * max_length * 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    677\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    678\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[0;32m--> 679\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-8089e151c8df>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, query, values)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# 마지막 dim을 input dim으로 간주하고 나머지 shape는 모두 유지: context_unit -> 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_with_time_axis\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# score shape: batch_size * in_seq_len * 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    677\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    678\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[0;32m--> 679\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1048\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(value, bias, data_format, name)\u001b[0m\n\u001b[1;32m   2658\u001b[0m       \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2659\u001b[0m       \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bias\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2660\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_nn_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(value, bias, data_format, name)\u001b[0m\n\u001b[1;32m    740\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"BiasAdd\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m         data_format)\n\u001b[0m\u001b[1;32m    743\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mhandle\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    838\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initializer_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0;34m\"\"\"The handle by which this variable can be accessed.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DLP1BX2pdbl",
        "outputId": "36a1cc88-98ea-4dfb-8a4c-271d112a0d6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "q, a = gen_test(1)\n",
        "result, sen, attention_plot = evaluate(q[0], encoder, decoder, char_indices, indices_char, in_seq_len, out_seq_len, enc_units, dec_units, context_units)\n",
        "plot_attention(attention_plot, list(padding(sen, in_seq_len)), list(padding(result, out_seq_len)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAJFCAYAAAA1V0iFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}
