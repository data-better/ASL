{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "12ê°•-transformer_text_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/data-better/ASL/blob/master/12%EA%B0%95_transformer_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqBXPLqIX8_V"
      },
      "source": [
        "## Transfomer implementation\n",
        "  - https://github.com/suyash/transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApTKI_a9Q0uX"
      },
      "source": [
        "import os\n",
        "\n",
        "from absl import app, flags, logging\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model  # pylint: disable=import-error\n",
        "from tensorflow.keras.callbacks import TensorBoard  # pylint: disable=import-error\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling1D, Input  # pylint: disable=import-error\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ha1M4OiREZw"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Add, Dense, Dropout, Embedding, Layer, LayerNormalization, Multiply, Permute, Reshape  # pylint: disable=import-error\n",
        "\n",
        "\n",
        "class Transformer:\n",
        "    def __init__(self,\n",
        "                 num_layers,\n",
        "                 d_model,\n",
        "                 num_heads,\n",
        "                 d_ff,\n",
        "                 input_vocab_size,\n",
        "                 target_vocab_size,\n",
        "                 dropout_rate,\n",
        "                 scope=\"transformer\"):\n",
        "        self.encoder = Encoder(num_layers=num_layers,\n",
        "                               d_model=d_model,\n",
        "                               num_heads=num_heads,\n",
        "                               d_ff=d_ff,\n",
        "                               vocab_size=input_vocab_size,\n",
        "                               dropout_rate=dropout_rate,\n",
        "                               scope=\"%s/encoder\" % scope)\n",
        "\n",
        "        self.decoder = Decoder(num_layers=num_layers,\n",
        "                               d_model=d_model,\n",
        "                               num_heads=num_heads,\n",
        "                               d_ff=d_ff,\n",
        "                               vocab_size=target_vocab_size,\n",
        "                               dropout_rate=dropout_rate,\n",
        "                               scope=\"%s/decoder\" % scope)\n",
        "\n",
        "        self.final_layer = Dense(target_vocab_size,\n",
        "                                 activation=None,\n",
        "                                 name=\"%s/dense\" % scope)\n",
        "\n",
        "        self.padding_mask = PaddingMask(name=\"%s/padding_mask\" % scope)\n",
        "        self.lookahead_mask = PaddingAndLookaheadMask(\n",
        "            name=\"%s/lookahead_mask\" % scope)\n",
        "\n",
        "    def __call__(self, inputs, target):\n",
        "        padding_mask = self.padding_mask(inputs)\n",
        "        lookahead_mask = self.lookahead_mask(target)\n",
        "\n",
        "        enc_output, enc_attention = self.encoder(inputs, padding_mask)\n",
        "\n",
        "        dec_output, dec_attention, enc_dec_attention = self.decoder(\n",
        "            target, enc_output, lookahead_mask, padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)\n",
        "\n",
        "        return final_output, enc_attention, dec_attention, enc_dec_attention\n",
        "\n",
        "\n",
        "class Decoder:\n",
        "    def __init__(self,\n",
        "                 num_layers,\n",
        "                 d_model,\n",
        "                 num_heads,\n",
        "                 d_ff,\n",
        "                 vocab_size,\n",
        "                 dropout_rate,\n",
        "                 scope=\"decoder\"):\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.scope = scope\n",
        "\n",
        "        self.embedding = Embedding(input_dim=vocab_size,\n",
        "                                   output_dim=d_model,\n",
        "                                   name=\"%s/embedding\" % scope)\n",
        "        self.pos_encoding = PositionalEncoding(d_model,\n",
        "                                               name=\"%s/positional_encoding\" %\n",
        "                                               scope)\n",
        "\n",
        "        self.dec_layers = [\n",
        "            DecoderLayer(d_model=d_model,\n",
        "                         num_heads=num_heads,\n",
        "                         d_ff=d_ff,\n",
        "                         dropout_rate=dropout_rate,\n",
        "                         scope=\"%s/decoder_layer_%d\" % (scope, i))\n",
        "            for i in range(num_layers)\n",
        "        ]\n",
        "\n",
        "        self.dropout = Dropout(dropout_rate, name=\"%s/dropout\" % self.scope)\n",
        "\n",
        "    def __call__(self, x, enc_output, lookahead_mask, padding_mask):\n",
        "        x = self.embedding(x)\n",
        "        x = MultiplyConstant(self.d_model, name=\"%s/multiply\" % self.scope)(x)\n",
        "        x = Add(name=\"%s/add\" % self.scope)([x, self.pos_encoding(x)])\n",
        "\n",
        "        dec_attention_weights = {}\n",
        "        enc_dec_attention_weights = {}\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, dec_attention, enc_dec_attention = self.dec_layers[i](\n",
        "                x, enc_output, lookahead_mask, padding_mask)\n",
        "\n",
        "            dec_attention_weights[\"layer_%d\" % i] = dec_attention\n",
        "            enc_dec_attention_weights[\"layer_%d\" % i] = enc_dec_attention\n",
        "\n",
        "        return x, dec_attention_weights, enc_dec_attention_weights\n",
        "\n",
        "\n",
        "class Encoder:\n",
        "    def __init__(self,\n",
        "                 num_layers,\n",
        "                 d_model,\n",
        "                 num_heads,\n",
        "                 d_ff,\n",
        "                 vocab_size,\n",
        "                 dropout_rate,\n",
        "                 scope=\"encoder\"):\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.scope = scope\n",
        "\n",
        "        self.embedding = Embedding(input_dim=vocab_size,\n",
        "                                   output_dim=d_model,\n",
        "                                   name=\"%s/embedding\" % scope)\n",
        "        self.pos_encoding = PositionalEncoding(d_model,\n",
        "                                               name=\"%s/positional_encoding\" %\n",
        "                                               scope)\n",
        "\n",
        "        self.enc_layers = [\n",
        "            EncoderLayer(d_model=d_model,\n",
        "                         num_heads=num_heads,\n",
        "                         d_ff=d_ff,\n",
        "                         dropout_rate=dropout_rate,\n",
        "                         scope=\"%s/encoder_layer_%d\" % (scope, i))\n",
        "            for i in range(num_layers)\n",
        "        ]\n",
        "\n",
        "        self.dropout = Dropout(dropout_rate, name=\"%s/dropout\" % self.scope)\n",
        "\n",
        "    def __call__(self, x, padding_mask):\n",
        "        x = self.embedding(x)\n",
        "        x = MultiplyConstant(self.d_model, name=\"%s/multiply\" % self.scope)(x)\n",
        "        x = Add(name=\"%s/add\" % self.scope)([x, self.pos_encoding(x)])\n",
        "\n",
        "        enc_attention_weights = {}\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, enc_attention = self.enc_layers[i](x, padding_mask)\n",
        "            enc_attention_weights[\"layer_%d\" % i] = enc_attention\n",
        "\n",
        "        return x, enc_attention_weights\n",
        "\n",
        "\n",
        "class DecoderLayer:\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 num_heads,\n",
        "                 d_ff,\n",
        "                 dropout_rate,\n",
        "                 scope=\"decoder_layer\"):\n",
        "        self.scope = scope\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model,\n",
        "                                       num_heads,\n",
        "                                       scope=\"%s/multi_head_attention_1\" %\n",
        "                                       scope)\n",
        "        self.mha2 = MultiHeadAttention(d_model,\n",
        "                                       num_heads,\n",
        "                                       scope=\"%s/multi_head_attention_2\" %\n",
        "                                       scope)\n",
        "        self.ffn = PointwiseFeedForwardNetwork(\n",
        "            d_model, d_ff, scope=\"%s/pointwise_feed_forward_network\" % scope)\n",
        "\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6,\n",
        "                                             name=\"%s/layer_norm_1\" % scope)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6,\n",
        "                                             name=\"%s/layer_norm_2\" % scope)\n",
        "        self.layernorm3 = LayerNormalization(epsilon=1e-6,\n",
        "                                             name=\"%s/layer_norm_3\" % scope)\n",
        "\n",
        "        self.dropout1 = Dropout(dropout_rate, name=\"%s/dropout_1\" % scope)\n",
        "        self.dropout2 = Dropout(dropout_rate, name=\"%s/dropout_2\" % scope)\n",
        "        self.dropout3 = Dropout(dropout_rate, name=\"%s/dropout_3\" % scope)\n",
        "\n",
        "    def __call__(self, x, enc_output, lookahead_mask, padding_mask):\n",
        "        out1, dec_dec_attention = self.mha1(x, x, x, lookahead_mask)\n",
        "        out1 = self.dropout1(out1)\n",
        "        x = Add(name=\"%s/add_1\" % self.scope)([x, out1])\n",
        "        x = self.layernorm1(x)\n",
        "\n",
        "        out2, enc_dec_attention = self.mha2(x, enc_output, enc_output,\n",
        "                                            padding_mask)\n",
        "        out2 = self.dropout2(out2)\n",
        "        x = Add(name=\"%s/add_2\" % self.scope)([x, out2])\n",
        "        x = self.layernorm2(x)\n",
        "\n",
        "        ffn_output = self.ffn(x)\n",
        "        ffn_output = self.dropout3(ffn_output)\n",
        "        x = Add(name=\"%s/add_3\" % self.scope)([x, ffn_output])\n",
        "        x = self.layernorm3(x)\n",
        "\n",
        "        return x, dec_dec_attention, enc_dec_attention\n",
        "\n",
        "\n",
        "class EncoderLayer:\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 num_heads,\n",
        "                 d_ff,\n",
        "                 dropout_rate,\n",
        "                 scope=\"encoder_layer\"):\n",
        "        self.scope = scope\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model,\n",
        "                                       num_heads,\n",
        "                                       scope=\"%s/multi_head_attention_1\" %\n",
        "                                       scope)\n",
        "        self.ffn = PointwiseFeedForwardNetwork(\n",
        "            d_model, d_ff, scope=\"%s/pointwise_feed_forward_network\" % scope)\n",
        "\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6,\n",
        "                                             name=\"%s/layer_norm_1\" % scope)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6,\n",
        "                                             name=\"%s/layer_norm_2\" % scope)\n",
        "\n",
        "        self.dropout1 = Dropout(dropout_rate, name=\"%s/dropout_1\" % scope)\n",
        "        self.dropout2 = Dropout(dropout_rate, name=\"%s/dropout_2\" % scope)\n",
        "\n",
        "    def __call__(self, x, padding_mask):\n",
        "        out1, enc_enc_attention = self.mha1(x, x, x, padding_mask)\n",
        "        out1 = self.dropout1(out1)\n",
        "        x = Add(name=\"%s/add_1\" % self.scope)([x, out1])\n",
        "        x = self.layernorm1(x)\n",
        "\n",
        "        ffn_output = self.ffn(x)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        x = Add(name=\"%s/add_2\" % self.scope)([x, ffn_output])\n",
        "        x = self.layernorm2(x)\n",
        "\n",
        "        return x, enc_enc_attention\n",
        "\n",
        "\n",
        "class PointwiseFeedForwardNetwork:\n",
        "    def __init__(self, d_model, d_ff, scope=\"pointwise_feed_forward_network\"):\n",
        "        self.dense_1 = Dense(d_ff,\n",
        "                             activation=\"relu\",\n",
        "                             name=\"%s/dense_1\" % scope)\n",
        "        self.dense_2 = Dense(d_model,\n",
        "                             activation=None,\n",
        "                             name=\"%s/dense_2\" % scope)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.dense_2(self.dense_1(x))\n",
        "\n",
        "\n",
        "class MultiHeadAttention:\n",
        "    def __init__(self, d_model, num_heads, scope=\"multi_head_attention\"):\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        self.wq = Dense(d_model, name=\"%s/dense_q\" % scope)\n",
        "        self.wk = Dense(d_model, name=\"%s/dense_k\" % scope)\n",
        "        self.wv = Dense(d_model, name=\"%s/dense_v\" % scope)\n",
        "\n",
        "        self.reshapeq = Reshape((-1, num_heads, d_model // num_heads),\n",
        "                                name=\"%s/reshape_q\" % scope)\n",
        "        self.reshapek = Reshape((-1, num_heads, d_model // num_heads),\n",
        "                                name=\"%s/reshape_k\" % scope)\n",
        "        self.reshapev = Reshape((-1, num_heads, d_model // num_heads),\n",
        "                                name=\"%s/reshape_v\" % scope)\n",
        "\n",
        "        self.transposeq = Permute((2, 1, 3), name=\"%s/transpose_q\" % scope)\n",
        "        self.transposek = Permute((2, 1, 3), name=\"%s/transpose_k\" % scope)\n",
        "        self.transposev = Permute((2, 1, 3), name=\"%s/transpose_v\" % scope)\n",
        "\n",
        "        self.reshape_output = Reshape((-1, d_model),\n",
        "                                      name=\"%s/reshape_output\" % scope)\n",
        "\n",
        "        self.transpose_output = Permute((2, 1, 3),\n",
        "                                        name=\"%s/transpose_output\" % scope)\n",
        "\n",
        "        self.dense = Dense(d_model, name=\"%s/dense\" % scope)\n",
        "\n",
        "        self.attention = Attention(name=\"%s/attention\" % scope)\n",
        "\n",
        "    def __call__(self, q, k, v, mask):\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        q = self.reshapeq(q)\n",
        "        k = self.reshapek(k)\n",
        "        v = self.reshapev(v)\n",
        "\n",
        "        q = self.transposeq(q)\n",
        "        k = self.transposek(k)\n",
        "        v = self.transposev(v)\n",
        "\n",
        "        x, attention_weights = self.attention([q, k, v, mask])\n",
        "\n",
        "        x = self.transpose_output(x)\n",
        "        x = self.reshape_output(x)\n",
        "        x = self.dense(x)\n",
        "\n",
        "        return x, attention_weights\n",
        "\n",
        "\n",
        "class Attention(Layer):\n",
        "    def call(self, inputs):\n",
        "        q, k, v, mask = inputs\n",
        "\n",
        "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "        scaled_attention_logits += mask * -1e9\n",
        "\n",
        "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "        output = tf.matmul(attention_weights, v)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "\n",
        "class PositionalEncoding(Layer):\n",
        "    def __init__(self, d_model, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def call(self, inputs):\n",
        "        position = tf.shape(inputs)[1]\n",
        "\n",
        "        position_dims = tf.range(position)[:, tf.newaxis]\n",
        "        embed_dims = tf.range(self.d_model)[tf.newaxis, :]\n",
        "        angle_rates = 1 / tf.pow(\n",
        "            10000.0, tf.cast(\n",
        "                (2 * (embed_dims // 2)) / self.d_model, tf.float32))\n",
        "        angle_rads = tf.cast(position_dims, tf.float32) * angle_rates\n",
        "\n",
        "        sines = tf.sin(angle_rads[:, 0::2])\n",
        "        cosines = tf.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "        return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "    def get_config(self):\n",
        "        base = super().get_config()\n",
        "        return dict(list(base.items()) + [(\"d_model\", self.d_model)])\n",
        "\n",
        "\n",
        "class MultiplyConstant(Layer):\n",
        "    def __init__(self, c, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.c = c\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return inputs * self.c\n",
        "\n",
        "    def get_config(self):\n",
        "        base = super().get_config()\n",
        "        return dict(list(base.items()) + [(\"c\", self.c)])\n",
        "\n",
        "\n",
        "class PaddingMask(Layer):\n",
        "    def call(self, inputs):\n",
        "        seq = tf.cast(tf.math.equal(inputs, 0), tf.float32)\n",
        "        return seq[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "\n",
        "class PaddingAndLookaheadMask(Layer):\n",
        "    def call(self, inputs):\n",
        "        size = tf.shape(inputs)[1]\n",
        "        lhm = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "\n",
        "        seq = tf.cast(tf.math.equal(inputs, 0), tf.float32)\n",
        "        seq = seq[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "        return tf.maximum(lhm, seq)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUfhFWvaRJgW"
      },
      "source": [
        "def main(_):\n",
        "    data, info = tfds.load(\"imdb_reviews/subwords8k\",\n",
        "                           with_info=True,\n",
        "                           as_supervised=True,\n",
        "                           data_dir=flags.FLAGS.tfds_data_dir)\n",
        "\n",
        "    train_data, test_data = data[tfds.Split.TRAIN], data[tfds.Split.TEST]\n",
        "\n",
        "    train_data = train_data.filter(\n",
        "        lambda x, y: tf.shape(x)[0] < flags.FLAGS.max_len)\n",
        "    train_data = train_data \\\n",
        "        .padded_batch(flags.FLAGS.batch_size, train_data.output_shapes) \\\n",
        "        .shuffle(flags.FLAGS.shuffle_buffer_size) \\\n",
        "        .repeat()\n",
        "\n",
        "    test_data = test_data.filter(\n",
        "        lambda x, y: tf.shape(x)[0] < flags.FLAGS.max_len)\n",
        "    test_data = test_data \\\n",
        "        .padded_batch(flags.FLAGS.batch_size, test_data.output_shapes)\n",
        "\n",
        "    vocab_size = info.features[\"text\"].encoder.vocab_size\n",
        "\n",
        "    inp = Input((None, ), dtype=\"int32\", name=\"inp\")\n",
        "    mask = PaddingMask()(inp)\n",
        "    net, enc_enc_attention_weights = Encoder(\n",
        "        num_layers=flags.FLAGS.num_layers,\n",
        "        d_model=flags.FLAGS.d_model,\n",
        "        num_heads=flags.FLAGS.num_heads,\n",
        "        d_ff=flags.FLAGS.d_ff,\n",
        "        vocab_size=vocab_size,\n",
        "        dropout_rate=flags.FLAGS.dropout_rate)(inp, mask)\n",
        "    net = GlobalAveragePooling1D()(net)\n",
        "    net = Dense(1, activation=\"sigmoid\")(net)\n",
        "\n",
        "    learning_rate = CustomSchedule(flags.FLAGS.d_model)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    loss_object = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "    if flags.FLAGS.use_custom_training_loop:\n",
        "        model = Model(inputs=inp, outputs=[net, enc_enc_attention_weights])\n",
        "        model.summary()\n",
        "\n",
        "        train(train_data=train_data,\n",
        "              validation_data=test_data,\n",
        "              model=model,\n",
        "              loss_object=loss_object,\n",
        "              optimizer=optimizer,\n",
        "              max_steps=flags.FLAGS.epochs * flags.FLAGS.steps_per_epoch,\n",
        "              save_summary_steps=flags.FLAGS.steps_per_epoch,\n",
        "              validation_steps=flags.FLAGS.validation_steps,\n",
        "              job_dir=flags.FLAGS[\"job-dir\"].value)\n",
        "    else:\n",
        "        model = Model(inputs=inp, outputs=net)\n",
        "        model.summary()\n",
        "\n",
        "        model.compile(optimizer=optimizer,\n",
        "                      loss=loss_object,\n",
        "                      metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "        model.fit(train_data,\n",
        "                  epochs=flags.FLAGS.epochs,\n",
        "                  steps_per_epoch=flags.FLAGS.steps_per_epoch,\n",
        "                  validation_data=test_data,\n",
        "                  validation_steps=flags.FLAGS.validation_steps,\n",
        "                  callbacks=[\n",
        "                      TensorBoard(log_dir=flags.FLAGS[\"job-dir\"].value),\n",
        "                  ])\n",
        "\n",
        "    model.save(os.path.join(flags.FLAGS[\"job-dir\"].value, \"model\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jG1AL_fRnId"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps**-1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "    def get_config(self):\n",
        "        return dict([(\"d_model\", self.d_model.numpy()),\n",
        "                     (\"warmup_steps\", self.warmup_steps)])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98jwkFmDRqQO"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, tar, model, loss_object, optimizer, loss_mean, acc):\n",
        "    with tf.GradientTape() as tape:\n",
        "        out, _ = model(inp, training=True)\n",
        "        loss = loss_object(y_true=tf.expand_dims(tar, 1), y_pred=out)\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    loss_mean(loss)\n",
        "    acc(y_true=tar, y_pred=out)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qi7BkOa1RsbX"
      },
      "source": [
        "def train(train_data, validation_data, model, loss_object, optimizer,\n",
        "          max_steps, save_summary_steps, validation_steps, job_dir):\n",
        "    loss_mean = tf.keras.metrics.Mean()\n",
        "    acc = tf.keras.metrics.BinaryAccuracy()\n",
        "\n",
        "    with tf.summary.create_file_writer(job_dir).as_default():  # pylint: disable=not-context-manager\n",
        "        for step, (inputs, outputs) in enumerate(train_data):\n",
        "            train_step(inputs,\n",
        "                       outputs,\n",
        "                       model=model,\n",
        "                       loss_object=loss_object,\n",
        "                       optimizer=optimizer,\n",
        "                       loss_mean=loss_mean,\n",
        "                       acc=acc)\n",
        "\n",
        "            if step % save_summary_steps == 0:\n",
        "                logging.info(\"Step: %d: Loss: %f, Accuracy: %f\", step,\n",
        "                             loss_mean.result(), acc.result())\n",
        "                tf.summary.scalar(\"Train Loss\", loss_mean.result(), step=step)\n",
        "                tf.summary.scalar(\"Train Accuracy\", acc.result(), step=step)\n",
        "\n",
        "                loss_mean.reset_states()\n",
        "                acc.reset_states()\n",
        "\n",
        "                current_validation_step = 0\n",
        "                for current_validation_step, (\n",
        "                        x, y_true) in enumerate(validation_data):\n",
        "                    y_pred, _ = model(x, training=False)\n",
        "                    loss = loss_object(y_true=tf.expand_dims(y_true, 1),\n",
        "                                       y_pred=y_pred)\n",
        "                    loss_mean(loss)\n",
        "                    acc(y_true, y_pred)\n",
        "\n",
        "                    if current_validation_step >= validation_steps:\n",
        "                        break\n",
        "\n",
        "                logging.info(\n",
        "                    \"Step: %d, validation_loss: %f, validation accuracy: %f\",\n",
        "                    step, loss_mean.result(), acc.result())\n",
        "                tf.summary.scalar(\"Validation Loss\",\n",
        "                                  loss_mean.result(),\n",
        "                                  step=step)\n",
        "                tf.summary.scalar(\"Validation Accuracy\",\n",
        "                                  acc.result(),\n",
        "                                  step=step)\n",
        "                loss_mean.reset_states()\n",
        "                acc.reset_states()\n",
        "\n",
        "            if step >= max_steps:\n",
        "                break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fVLWPAfRur_",
        "outputId": "e8776cff-89f4-491d-e1da-29b102625d3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "flags = app.flags\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "app.flags.DEFINE_string('f', '', 'kernel')\n",
        "  \n",
        "app.flags.DEFINE_integer(\"d_model\", 128, \"d_model\")\n",
        "app.flags.DEFINE_integer(\"d_ff\", 512, \"d_ff\")\n",
        "app.flags.DEFINE_integer(\"num_layers\", 2, \"num_layers\")\n",
        "app.flags.DEFINE_integer(\"num_heads\", 8, \"num_heads\")\n",
        "app.flags.DEFINE_float(\"dropout_rate\", 0.1, \"dropout_rate\")\n",
        "app.flags.DEFINE_integer(\"epochs\", 50, \"epochs\")\n",
        "app.flags.DEFINE_integer(\"steps_per_epoch\", 250, \"steps_per_epoch\")\n",
        "app.flags.DEFINE_integer(\"max_len\", 500, \"max_len\")\n",
        "app.flags.DEFINE_integer(\"batch_size\", 64, \"batch_size\")\n",
        "app.flags.DEFINE_integer(\"shuffle_buffer_size\", 500, \"shuffle_buffer_size\")\n",
        "app.flags.DEFINE_integer(\"validation_steps\", 50, \"validation_steps\")\n",
        "app.flags.DEFINE_boolean(\"use_custom_training_loop\", False,\n",
        "                         \"use_custom_training_loop\")\n",
        "app.flags.DEFINE_string(\"tfds_data_dir\", \"~/tensorflow_datasets\",\n",
        "                        \"tfds_data_dir\")\n",
        "app.flags.DEFINE_string(\"job-dir\", \"runs/text_classification\", \"job-dir\")\n",
        "app.run(main)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I0811 01:52:24.725407 140363197581184 dataset_builder.py:184] Overwrite dataset info from restored data version.\n",
            "I0811 01:52:24.856547 140363197581184 dataset_builder.py:253] Reusing dataset imdb_reviews (/root/tensorflow_datasets/imdb_reviews/subwords8k/0.1.0)\n",
            "I0811 01:52:24.857880 140363197581184 dataset_builder.py:399] Constructing tf.data.Dataset for split None, from /root/tensorflow_datasets/imdb_reviews/subwords8k/0.1.0\n",
            "W0811 01:52:24.919736 140363197581184 dataset_builder.py:439] Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n",
            "W0811 01:52:25.019160 140363197581184 deprecation.py:323] From <ipython-input-3-1476b29ac834>:11: DatasetV1.output_shapes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_shapes(dataset)`.\n",
            "W0811 01:52:25.128939 140363197581184 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0811 01:52:25.131566 140363197581184 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0811 01:52:26.229171 140363197581184 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "inp (InputLayer)                [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder/embedding (Embedding)   (None, None, 128)    1047680     inp[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "encoder/multiply (MultiplyConst (None, None, 128)    0           encoder/embedding[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "encoder/positional_encoding (Po (1, None, 128)       0           encoder/multiply[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "encoder/add (Add)               (None, None, 128)    0           encoder/multiply[0][0]           \n",
            "                                                                 encoder/positional_encoding[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_0/multi_h (None, None, 128)    16512       encoder/add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_0/multi_h (None, None, 128)    16512       encoder/add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_0/multi_h (None, None, 128)    16512       encoder/add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_0/multi_h (None, None, 8, 16)  0           encoder/encoder_layer_0/multi_hea\n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_0/multi_h (None, None, 8, 16)  0           encoder/encoder_layer_0/multi_hea\n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_0/multi_h (None, None, 8, 16)  0           encoder/encoder_layer_0/multi_hea\n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_0/multi_h (None, 8, None, 16)  0           encoder/encoder_layer_0/multi_hea\n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_0/multi_h (None, 8, None, 16)  0           encoder/encoder_layer_0/multi_hea\n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_0/multi_h (None, 8, None, 16)  0           encoder/encoder_layer_0/multi_hea\n",
            "__________________________________________________________________________________________________\n",
            "padding_mask (PaddingMask)      (None, 1, 1, None)   0           inp[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_0/multi_h ((None, 8, None, 16) 0           encoder/encoder_layer_0/multi_hea\n",
            "                                                                 encoder/encoder_layer_0/multi_hea\n",
            "                                                                 encoder/encoder_layer_0/multi_hea\n",
            "                                                                 padding_mask[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_0/multi_h (None, None, 8, 16)  0           encoder/encoder_layer_0/multi_hea\n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_0/multi_h (None, None, 128)    0           encoder/encoder_layer_0/multi_hea\n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_0/multi_h (None, None, 128)    16512       encoder/encoder_layer_0/multi_hea\n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_0/dropout (None, None, 128)    0           encoder/encoder_layer_0/multi_hea\n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_0/add_1 ( (None, None, 128)    0           encoder/add[0][0]                \n",
            "                                                                 encoder/encoder_layer_0/dropout_1\n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_0/layer_n (None, None, 128)    256         encoder/encoder_layer_0/add_1[0][\n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_0/pointwi (None, None, 512)    66048       encoder/encoder_layer_0/layer_nor\n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_0/pointwi (None, None, 128)    65664       encoder/encoder_layer_0/pointwise\n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_0/dropout (None, None, 128)    0           encoder/encoder_layer_0/pointwise\n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_0/add_2 ( (None, None, 128)    0           encoder/encoder_layer_0/layer_nor\n",
            "                                                                 encoder/encoder_layer_0/dropout_2\n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_0/layer_n (None, None, 128)    256         encoder/encoder_layer_0/add_2[0][\n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_1/multi_h (None, None, 128)    16512       encoder/encoder_layer_0/layer_nor\n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_1/multi_h (None, None, 128)    16512       encoder/encoder_layer_0/layer_nor\n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_1/multi_h (None, None, 128)    16512       encoder/encoder_layer_0/layer_nor\n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_1/multi_h (None, None, 8, 16)  0           encoder/encoder_layer_1/multi_hea\n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_1/multi_h (None, None, 8, 16)  0           encoder/encoder_layer_1/multi_hea\n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_1/multi_h (None, None, 8, 16)  0           encoder/encoder_layer_1/multi_hea\n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_1/multi_h (None, 8, None, 16)  0           encoder/encoder_layer_1/multi_hea\n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_1/multi_h (None, 8, None, 16)  0           encoder/encoder_layer_1/multi_hea\n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_1/multi_h (None, 8, None, 16)  0           encoder/encoder_layer_1/multi_hea\n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_1/multi_h ((None, 8, None, 16) 0           encoder/encoder_layer_1/multi_hea\n",
            "                                                                 encoder/encoder_layer_1/multi_hea\n",
            "                                                                 encoder/encoder_layer_1/multi_hea\n",
            "                                                                 padding_mask[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_1/multi_h (None, None, 8, 16)  0           encoder/encoder_layer_1/multi_hea\n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_1/multi_h (None, None, 128)    0           encoder/encoder_layer_1/multi_hea\n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_1/multi_h (None, None, 128)    16512       encoder/encoder_layer_1/multi_hea\n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_1/dropout (None, None, 128)    0           encoder/encoder_layer_1/multi_hea\n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_1/add_1 ( (None, None, 128)    0           encoder/encoder_layer_0/layer_nor\n",
            "                                                                 encoder/encoder_layer_1/dropout_1\n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_1/layer_n (None, None, 128)    256         encoder/encoder_layer_1/add_1[0][\n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_1/pointwi (None, None, 512)    66048       encoder/encoder_layer_1/layer_nor\n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_1/pointwi (None, None, 128)    65664       encoder/encoder_layer_1/pointwise\n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_1/dropout (None, None, 128)    0           encoder/encoder_layer_1/pointwise\n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_1/add_2 ( (None, None, 128)    0           encoder/encoder_layer_1/layer_nor\n",
            "                                                                 encoder/encoder_layer_1/dropout_2\n",
            "__________________________________________________________________________________________________\n",
            "encoder/encoder_layer_1/layer_n (None, None, 128)    256         encoder/encoder_layer_1/add_2[0][\n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d (Globa (None, 128)          0           encoder/encoder_layer_1/layer_nor\n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            129         global_average_pooling1d[0][0]   \n",
            "==================================================================================================\n",
            "Total params: 1,444,353\n",
            "Trainable params: 1,444,353\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "250/250 [==============================] - 64s 257ms/step - loss: 0.6538 - binary_accuracy: 0.6024 - val_loss: 0.5242 - val_binary_accuracy: 0.7466\n",
            "Epoch 2/50\n",
            "250/250 [==============================] - 59s 238ms/step - loss: 0.3396 - binary_accuracy: 0.8519 - val_loss: 0.3705 - val_binary_accuracy: 0.8441\n",
            "Epoch 3/50\n",
            "250/250 [==============================] - 59s 236ms/step - loss: 0.2235 - binary_accuracy: 0.9091 - val_loss: 0.4428 - val_binary_accuracy: 0.8456\n",
            "Epoch 4/50\n",
            "250/250 [==============================] - 59s 236ms/step - loss: 0.1728 - binary_accuracy: 0.9330 - val_loss: 0.4973 - val_binary_accuracy: 0.8541\n",
            "Epoch 5/50\n",
            "250/250 [==============================] - 59s 236ms/step - loss: 0.1446 - binary_accuracy: 0.9439 - val_loss: 0.4028 - val_binary_accuracy: 0.8406\n",
            "Epoch 6/50\n",
            "250/250 [==============================] - 59s 236ms/step - loss: 0.1312 - binary_accuracy: 0.9481 - val_loss: 0.4932 - val_binary_accuracy: 0.8209\n",
            "Epoch 7/50\n",
            "250/250 [==============================] - 59s 235ms/step - loss: 0.1410 - binary_accuracy: 0.9441 - val_loss: 0.4663 - val_binary_accuracy: 0.8447\n",
            "Epoch 8/50\n",
            "250/250 [==============================] - 59s 236ms/step - loss: 0.1329 - binary_accuracy: 0.9487 - val_loss: 0.4734 - val_binary_accuracy: 0.8531\n",
            "Epoch 9/50\n",
            " 27/250 [==>...........................] - ETA: 47s - loss: 0.1376 - binary_accuracy: 0.9491"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-bcefa1ae9d9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                         \"tfds_data_dir\")\n\u001b[1;32m     21\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"job-dir\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"runs/text_classification\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"job-dir\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/absl/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv, flags_parser)\u001b[0m\n\u001b[1;32m    298\u001b[0m       \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m       \u001b[0m_run_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mUsageError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m       \u001b[0musage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshorthelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetailed_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexitcode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/absl/app.py\u001b[0m in \u001b[0;36m_run_main\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-1476b29ac834>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m     60\u001b[0m                   \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                   callbacks=[\n\u001b[0;32m---> 62\u001b[0;31m                       \u001b[0mTensorBoard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"job-dir\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                   ])\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m           \u001b[0;31m# `ins` can be callable in tf.distribute.Strategy + eager case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m           \u001b[0mactual_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mins\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFtyxoeRRytX"
      },
      "source": [
        "!ls ../"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2VHYjvqU7mS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}